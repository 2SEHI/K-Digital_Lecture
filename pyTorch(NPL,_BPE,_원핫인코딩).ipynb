{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch(NPL, BPE, 원핫인코딩).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyObN2tAvqz2ebBCH8rmUVi9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2SEHI/K-Digital_Lecture/blob/main/pyTorch(NPL%2C_BPE%2C_%EC%9B%90%ED%95%AB%EC%9D%B8%EC%BD%94%EB%94%A9).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27cQKDN3AVDO"
      },
      "source": [
        "# NPL\n",
        "- Natural Language Processing, 자연어 처리\n",
        "- NLP는 Text데이터를 분석하고 모델링하는 분야로 이를 다음과 같이 구분하기도 합니다.\n",
        "    - **자연어 이해(Natural Language Understanding)영역** : Text의 의미를 파악하는 것\n",
        "    - **자연어 생성(Natural Language Generation)영역** : 주어진 의미에 대한 자연스러운 Text를 만들어 내는 것\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g99lzW__bKDJ"
      },
      "source": [
        "\n",
        "## 1.NPL 분야\n",
        "- 감정분석\n",
        "- 요약\n",
        "    - 문장의 의미를 파악해서 새로운 문장을 만들어 내야 합니다\n",
        "- 기계 번역\n",
        "- 질의 응답 \n",
        "    - 현업에서는 API를 많이 활용합니다.\n",
        "    - 한국어 데이터 셋으로 korQuAD가 있습니다.\n",
        "- POS Tagging\n",
        "    - 품사 예측\n",
        "- 챗봇\n",
        "- 문장 간의 논리적인 관계에 대한 분류 모델\n",
        "    - GAN\n",
        "    - 소설, 이미지, 음악과 같은 창작의 영역에서 많이 사용됩니다.\n",
        "- Image Captioning \n",
        "    - 이미지 속 상황을 설명"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfcMyDwQb5eu"
      },
      "source": [
        "## 2.NPL 참고 링크 \n",
        "- PaperWithCodes: [https://paperswithcode.com/area/natural-language-processing](https://paperswithcode.com/area/natural-language-processing)\n",
        "- NLPPogress: [http://nlpprogress.com/](http://nlpprogress.com/)\n",
        "\n",
        "- SQuAD: [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/)\n",
        "\n",
        "- KorQuAD: [https://korquad.github.io/](https://korquad.github.io/)\n",
        "\n",
        "- CoQA: [https://stanfordnlp.github.io/coqa/](https://stanfordnlp.github.io/coqa/)\n",
        "\n",
        "- GLUE Benchmark: [https://gluebenchmark.com/](https://gluebenchmark.com/)\n",
        "\n",
        "- PaperWithCodes: [https://paperswithcode.com/area/natural-language-processing](https://paperswithcode.com/area/natural-language-processing)\n",
        "\n",
        "- NLPPogress: [http://nlpprogress.com/](http://nlpprogress.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1xo0Dd5cSlH"
      },
      "source": [
        "## 3.NPL 전처리 - 문자를 숫자로 표현하는 방법\n",
        "- 자연어는 숫자로 어떻게 표현해야 딥러닝에서 사용이 가능할까?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnR8o0N6dD9U"
      },
      "source": [
        "### 1) 문장을 숫자로 표현하기 위한 순서\n",
        "- Text Segmentation \n",
        "    - 문장을 의미있는 단위로 나누기\n",
        "- Representation \n",
        "    - 의미있는 부분을 숫자로 변환\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elzN3MoGdFl3"
      },
      "source": [
        "### 2) 데이터를 최소 단위로 분할\n",
        "- Tokenize라고 합니다\n",
        "- str 클래스의 split을 이용한 공백 단위 분할.\n",
        "- 글자 단위로 나누기\n",
        "    - 파이썬은 str이 글자의 list임\n",
        "    - list(문자열) : 글자 단위로 list 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dSg2jKmc-jQ"
      },
      "source": [
        "## 4.문자를 숫자로 표현하는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0mvAG-9dMqj"
      },
      "source": [
        "### 1) 샘플 문장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDHbrqpRHDfa"
      },
      "source": [
        "sample1 = '나와 형은 노래를 참 못한다'\n",
        "sample2 = '그래서 노래를 대신 불러 줄 수 있는 애가 필요했다.'\n",
        "sample3 = '아담을 만들었는데 얼굴도 잘 생겼으면 좋을 것 같아서 원빈을 모델로 변경했다.'\n",
        "sample4 = '노래 한 곡 하는데 천만원이 드는데 가요 톱텐 나가면 11만원을 줬다.'\n",
        "sample5 = '돈이 없어서 강진 축구와 사이버 컵을 버렸다.'\n",
        "sample6 = '가장 슬픈 날이었다.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PizV_9tHyuA"
      },
      "source": [
        "### 2)공백 단위로 분할\n",
        "- str클래스의 split을 이용한 공백 단위 분할\n",
        "- str의 split은 정규식을 기준으로 문자열을 분할하여 list를 생성하는데 정규식을 설정하지 않으면 공백 단위로 분할합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVO2USElH6Xv",
        "outputId": "298bb6b3-c4d1-465f-e2f7-e9711b425fd0"
      },
      "source": [
        "print(sample1.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['나와', '형은', '노래를', '참', '못한다']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BKiw97ZISwl"
      },
      "source": [
        "### 3) 데이터 최소단위로 나누기\n",
        "\n",
        "- 글자 단위로 나누기 \n",
        "- 파이썬은 str은 문자열의 list임을 이용합니다\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9y0NoKcIIP9",
        "outputId": "954855aa-9332-4693-b145-5158a35168a3"
      },
      "source": [
        "print(list(sample2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['그', '래', '서', ' ', '노', '래', '를', ' ', '대', '신', ' ', '불', '러', ' ', '줄', ' ', '수', ' ', '있', '는', ' ', '애', '가', ' ', '필', '요', '했', '다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMQP4vJpd4kc"
      },
      "source": [
        "### 4) 단어 수치화\n",
        "- 공백을 기준으로 분할한 단어들을 수치화 하기 \n",
        "- 중복된 단어는1번만 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYDG536gJOq5",
        "outputId": "ad09d4dc-ff43-44c7-bb6f-667c57e16854"
      },
      "source": [
        "token2idx = {} # 데이터를 저장할 딕셔너리 생성\n",
        "index = 0\n",
        "# 문장들을 순회\n",
        "for sentence in [sample1, sample2, sample3, sample4, sample5, sample6]:\n",
        "    # 각 문장을 공백 단위로 분할\n",
        "    tokens = sentence.split()\n",
        "    # 분할 한 문장의 단어를 순회\n",
        "    for token in tokens:\n",
        "        # dict에 존재하지 않으면 단어를 저장하고 index값을 1씩 증가시킴\n",
        "        if token2idx.get(token) == None:\n",
        "            token2idx[token] = index\n",
        "            index += 1\n",
        "\n",
        "print(token2idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'나와': 0, '형은': 1, '노래를': 2, '참': 3, '못한다': 4, '그래서': 5, '대신': 6, '불러': 7, '줄': 8, '수': 9, '있는': 10, '애가': 11, '필요했다.': 12, '아담을': 13, '만들었는데': 14, '얼굴도': 15, '잘': 16, '생겼으면': 17, '좋을': 18, '것': 19, '같아서': 20, '원빈을': 21, '모델로': 22, '변경했다.': 23, '노래': 24, '한': 25, '곡': 26, '하는데': 27, '천만원이': 28, '드는데': 29, '가요': 30, '톱텐': 31, '나가면': 32, '11만원을': 33, '줬다.': 34, '돈이': 35, '없어서': 36, '강진': 37, '축구와': 38, '사이버': 39, '컵을': 40, '버렸다.': 41, '가장': 42, '슬픈': 43, '날이었다.': 44}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7kAJh_uOErV"
      },
      "source": [
        "### 5) 문장을 수치화해주는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XxnwBySJO5M",
        "outputId": "4b142dd7-f2f4-4bbb-c03d-db0c49480634"
      },
      "source": [
        "# 문장을 수치화 해주는 함수\n",
        "def indexed_sentence(sentence):\n",
        "    # 단어하나씩 token2idx 딕셔터리의 key로 대입하여\n",
        "    # 저장된 value값을 리스트로 반환\n",
        "    return [token2idx[token] for token in sentence]\n",
        "\n",
        "# 문장을 단어 단위로 분할하여 리스트로 넘겨줌\n",
        "result = indexed_sentence(sample2.split())\n",
        "\n",
        "# 단어에 해당하는 숫자를 반환\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 2, 6, 7, 8, 9, 10, 11, 12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D1OV13iJPCJ"
      },
      "source": [
        "### 6) 위 방법의 문제점\n",
        "- 모델을 만들 때 사용하지 않은 단어가 포함되어 있으면 아래와 같이 에러가 발생합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "jAG-0MP7O_zE",
        "outputId": "da59d908-241b-4c28-ad84-4d3ba9651f1e"
      },
      "source": [
        "test_sentence = '형은 노래를 못하는 대신 축구를 잘한다'\n",
        "\n",
        "# 문장을 단어 단위로 분할하여 리스트로 넘겨줌\n",
        "result = indexed_sentence(test_sentence.split())\n",
        "\n",
        "# 단어에 해당하는 숫자를 반환\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-d4105292e56f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 문장을 단어 단위로 분할하여 리스트로 넘겨줌\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexed_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 단어에 해당하는 숫자를 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-d940e29ba9ab>\u001b[0m in \u001b[0;36mindexed_sentence\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# 단어하나씩 token2idx 딕셔터리의 key로 대입하여\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 저장된 value값을 리스트로 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 문장을 단어 단위로 분할하여 리스트로 넘겨줌\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-d940e29ba9ab>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# 단어하나씩 token2idx 딕셔터리의 key로 대입하여\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 저장된 value값을 리스트로 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 문장을 단어 단위로 분할하여 리스트로 넘겨줌\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '못하는'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI_KhvCAVfqT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKXmsZ3oPelU"
      },
      "source": [
        "## 5.OOV(Out-Of-Vocabulary)\n",
        "- 위 문제를 해결하는 방법 중 하나가 OOV를 위한 Token을 별도로 만드는 것입니다.\n",
        "- OOV Token을 만들어 사전에 없는 단어가 등장할 경우 OOV Token이 등장하도록 합니다.\n",
        "질문"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGvFyr5Ye0KN"
      },
      "source": [
        "### 1) OOV를 위한 별도의 토큰 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv0dI2txQQXh",
        "outputId": "6945ea0d-495e-4ab8-f3e5-a4b5aab8808f"
      },
      "source": [
        "test_sentence = '형은 노래를 못하는 대신 축구를 잘한다'\n",
        "\n",
        "# 튜플 딕셔너리 형태로 바꿔줌\n",
        "# 0부터 시작하는 형태에서 1부터 시작하는 형태로 바꿔줌\n",
        "token2idx = {t:i+1 for t, i in token2idx.items()}\n",
        "# OOV token추가\n",
        "token2idx['<unk>'] = 0\n",
        "\n",
        "# 문장을 수치화 해주는 함수인데 사전에 없는 단어는 <unk>토큰으로 등장\n",
        "def indexed_sentence_unk(sentence):\n",
        "    return [token2idx.get(token, token2idx['<unk>']) for token in sentence]\n",
        "\n",
        "print(indexed_sentence_unk(test_sentence.split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 3, 0, 7, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk4S_-_ipD9M",
        "outputId": "076cac71-8c8e-4067-d1c8-525699705146"
      },
      "source": [
        "token2idx.items()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('나와', 0), ('형은', 1), ('노래를', 2), ('참', 3), ('못한다', 4), ('그래서', 5), ('대신', 6), ('불러', 7), ('줄', 8), ('수', 9), ('있는', 10), ('애가', 11), ('필요했다.', 12), ('아담을', 13), ('만들었는데', 14), ('얼굴도', 15), ('잘', 16), ('생겼으면', 17), ('좋을', 18), ('것', 19), ('같아서', 20), ('원빈을', 21), ('모델로', 22), ('변경했다.', 23), ('노래', 24), ('한', 25), ('곡', 26), ('하는데', 27), ('천만원이', 28), ('드는데', 29), ('가요', 30), ('톱텐', 31), ('나가면', 32), ('11만원을', 33), ('줬다.', 34), ('돈이', 35), ('없어서', 36), ('강진', 37), ('축구와', 38), ('사이버', 39), ('컵을', 40), ('버렸다.', 41), ('가장', 42), ('슬픈', 43), ('날이었다.', 44)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPZ4EP4OSILf"
      },
      "source": [
        "## 6.Corpus\n",
        "- Token을 만들기 위해서 모아 놓은 문장의 모음을 Corpus라고 합니다.\n",
        "- 영어에 대한 Corpus는 [https://www.english-corpora.org/](https://www.english-corpora.org/)에서 확인이 가능합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2thaaMFmSoyS"
      },
      "source": [
        "## 7.BPE(Byte Pair Encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osq2Wu9sfDx4"
      },
      "source": [
        "### 1) 한국어의 단위\n",
        "```음운 -> 음절 -> 형태소 -> 단어 -> 어절 -> 문장```\n",
        "- 음운 : 소리의 단위\n",
        "- 음절 : 글자\n",
        "- 형태소 : 의미를 가진 최소 단위\n",
        "- 단어 : 최소의 자립 형식으로 일반적으로 공백을 기준으로 분할하는데 한글의 경우 조사가 붙을 수 있어 두개의 단어가 하나의 어절이 됩니다.\n",
        "- 어절 : 문장을 이루는 마디\n",
        "- 문장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqKS4mCWfPWH"
      },
      "source": [
        "#### 🎈영어와 한국어의 차이\n",
        "- 영어는 단어와 어절이 같은데 한국어는 그렇지 않습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--4h9CldfbHU"
      },
      "source": [
        "### 2) 공백이 아닌 글자 단위로 사전 만들기\n",
        "- 공백이 아닌 글자 단위로 사전 만들면 사전의 크기가 줄어들고 OOV문제가 해결됩니다.\n",
        "- 글자는 의미를 갖지 않는 경우가 대부분이라서 직접 사용하는 것이 어렵습니다.\n",
        "- 그래서 생각해 낸 방법이 n-gram  Tokenization입니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50H01o0kflM0"
      },
      "source": [
        "### 3) n-gram Tokenization\n",
        "- 글자 단위 뿐 아니라 글자의 결합에도 관심을 갖는 방법\n",
        "- 하나의 글자만 이용하는 경우를 uni-gram이라고 하고 2개의 글자를 이용하는 것을 bi-gram이라고 합니다.\n",
        "- 한 번만 사용하는 단어라던가 이후에 아예 등장하지 않는 단어들도 사전에 등록해야 해서 이 방법을 사용하게 되면 단어 사전이 너무 커집니다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAoMIAjsfmX9"
      },
      "source": [
        "### 4) BPE\n",
        "- n-gram중에서 사용이 될 것 같은 데이터만 모은 것입니다.\n",
        "- BPE는 반복적으로 나오는 데이터의 패턴을 치환하는 방식을 사용하여 데이터를 효율적으로 저장하기 위한 압축 알고리즘입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAZ7r-XRf1gP"
      },
      "source": [
        "### 5) BPE 순서\n",
        "문장 \"abbcabcab\"가 있다고 할 때\n",
        "```\n",
        "    1.XbcXcX  - 가장 많이 등장하는 2개의 패턴ab을 찾아서 X로 치환\n",
        "        \n",
        "    2.XbYY -  cX가 두번 등장하므로 cX를 Y로 치환\n",
        "```\n",
        "- 이러한 작업을 더이상 변경이 불가능할 때까지 또는 일정한 횟수만큼 수행하는 방식입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82jFlzibfzMP"
      },
      "source": [
        "## 8.BPE 구현\n",
        "\n",
        "- defaultdict : 기본 딕셔너리는 해당 키가 없는 값을 출력할 경우 KeyError Exception 에러가 나지만, defaultdict는 초기값(default)를 지정할 수 있습니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJJ1YyXiggwt"
      },
      "source": [
        "### 1) 단어 조합에 대한 등장 횟수를 dict에 저장하여 반환하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF0kKLxpSovg"
      },
      "source": [
        "import re, collections\n",
        "\n",
        "def get_stats(vocab):\n",
        "    # defaultdict : 딕셔너리를 만드는 dict클래스의 서브클래스\n",
        "    # int설정시 default count of zero\n",
        "    pairs = collections.defaultdict(int)\n",
        "    # print(vocab.items())\n",
        "    for word, freq in vocab.items():\n",
        "        # 문장을 공백단위로 분할\n",
        "        symbols = word.split()\n",
        "        # print(symbols)\n",
        "        # 모든 단어의 조합에 대한 횟수를 세서 dict에 저장\n",
        "        for i in range(len(symbols)-1):\n",
        "            # 횟수 세기\n",
        "            pairs[symbols[i], symbols[i+1]] += freq\n",
        "    return pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLmLvIM2gb2m"
      },
      "source": [
        "### 2) 단어 조합사이의 공백을 제거해주는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8Jt697YgdM_"
      },
      "source": [
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    # print(bigram)\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    # print(p)\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiRDGa49SosV",
        "outputId": "2c02d1fe-9173-4c3c-be72-6107a391ef05"
      },
      "source": [
        "# 테스트할 단어 생성\n",
        "# 단어, 빈도수\n",
        "vocab = ({'l o w </w>':5, 'l o w e r </w>':2, 'n e w e s t </w>':6, 'w i d e s t </w>':3})\n",
        "\n",
        "# 몇 번 수행할 것인지\n",
        "num_merges = 10\n",
        "\n",
        "for i in range(num_merges):\n",
        "    # 단어 조합에 대한 등장 횟수를 dict에 저장하여 반환\n",
        "    pairs = get_stats(vocab)\n",
        "    \n",
        "    # 가장 많이 등장한 단어의 조합\n",
        "    # pairs.get는 단어조합의 등장 빈도\n",
        "    best = max(pairs, key = pairs.get)\n",
        "\n",
        "    # 가장 많이 등장한 단어 조합의 중간 공백을 제거합니다.\n",
        "    vocab = merge_vocab(best, vocab)\n",
        "    print(f'Step {i+1}')\n",
        "    print(best)\n",
        "    print(vocab)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1\n",
            "('e', 's')\n",
            "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
            "\n",
            "\n",
            "Step 2\n",
            "('es', 't')\n",
            "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
            "\n",
            "\n",
            "Step 3\n",
            "('est', '</w>')\n",
            "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 4\n",
            "('l', 'o')\n",
            "{'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 5\n",
            "('lo', 'w')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 6\n",
            "('n', 'e')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 7\n",
            "('ne', 'w')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 8\n",
            "('new', 'est</w>')\n",
            "{'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 9\n",
            "('low', '</w>')\n",
            "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
            "\n",
            "\n",
            "Step 10\n",
            "('w', 'i')\n",
            "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4jJJbWv6XeO"
      },
      "source": [
        "## 9.One Hot Encoding\n",
        "- 범주형 데이터(Factor - 정해진 값 내에서 선택하는 형태의 데이터)를 숫자로 표현하는 방법 중 하나가 One Hot Encoding이라고 하는데 이렇게 One Hot Encoding된 데이터를 통계학에서는 Dummy Variable이라고 합니다. \n",
        "- 자연어 처리에서는 단어 사전 크기 만큼의 0값을 가진 0벡터를 만들고 Token에 해당하는 index에만 1을 대입하는 형태로 생성합니다.\n",
        "- 실제로 이 방법은 자연어 처리에서 잘 쓰이지 않는데 통계 분석에서 순서가 필요 없는 범주형 데이터를 표현하고자 할 때만 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCiB5YnogwW5"
      },
      "source": [
        "## 10.기본적인 원 핫 인코딩 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzSv4gfjhL3u"
      },
      "source": [
        "### 1) 단어사전 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88PDPvhl85au",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f76608-35f3-4e62-bc91-d62de9f34856"
      },
      "source": [
        "s1 = \"나는 처음에 어셈블리 언어를 배웠는데 재미가 너무 없었다\"\n",
        "s2 = '두번째 배운 언어는 코볼이었는데 그나마 조금 나았다'\n",
        "s3 = '나는 데이터베이스를 처음 배웠을 때 황당했다'\n",
        "s4 = 'S/W 공학이 제일 재미있는 과목이었다'\n",
        "s5 = '세상에서 포토샵이 제일 싫었다'\n",
        "s6 = '요즈음에는 ML_OPS 나 Metaverse 에 관심이 많다'\n",
        "\n",
        "# 단어 사전 만들기\n",
        "token2idx = {}\n",
        "index = 0\n",
        "\n",
        "# 문장 순회\n",
        "for sentence in [s1, s2, s3, s4, s5, s6]:\n",
        "    # 공백단위의 단어 리스트만들기\n",
        "    tokens = sentence.split()\n",
        "    # 단어 순회\n",
        "    for token in tokens:\n",
        "        # 단어 사전에 단어가 존재하지 않으면 단어사전에 등록\n",
        "        if token2idx.get(token) == None:\n",
        "            token2idx[token] = index\n",
        "            index += 1\n",
        "\n",
        "print(token2idx)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'나는': 0, '처음에': 1, '어셈블리': 2, '언어를': 3, '배웠는데': 4, '재미가': 5, '너무': 6, '없었다': 7, '두번째': 8, '배운': 9, '언어는': 10, '코볼이었는데': 11, '그나마': 12, '조금': 13, '나았다': 14, '데이터베이스를': 15, '처음': 16, '배웠을': 17, '때': 18, '황당했다': 19, 'S/W': 20, '공학이': 21, '제일': 22, '재미있는': 23, '과목이었다': 24, '세상에서': 25, '포토샵이': 26, '싫었다': 27, '요즈음에는': 28, 'ML_OPS': 29, '나': 30, 'Metaverse': 31, '에': 32, '관심이': 33, '많다': 34}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj14IRD1g4H5"
      },
      "source": [
        "### 2) 원핫인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O3RvNI_O_Kq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29c5d841-cf2a-4e23-c8a5-ae88d6998ad8"
      },
      "source": [
        "# 단어 사전의 길이를 저장\n",
        "V = len(token2idx)\n",
        "\n",
        "# 단어 길이 만큼의 list를 생성하는데 기본적으로 0을 저장하고 \n",
        "# 자신의 index위치에 1을 삽입\n",
        "token2vec = [([0 if i != idx else 1 for i in range(V)], idx, token) for token, idx in token2idx.items()]\n",
        "# print(token2vec)\n",
        "\n",
        "for x in token2vec:\n",
        "    print('\\t'.join([str(y) for y in x]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t0\t나는\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t1\t처음에\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t2\t어셈블리\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t3\t언어를\n",
            "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t4\t배웠는데\n",
            "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t5\t재미가\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t6\t너무\n",
            "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t7\t없었다\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t8\t두번째\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t9\t배운\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t10\t언어는\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t11\t코볼이었는데\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t12\t그나마\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t13\t조금\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t14\t나았다\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t15\t데이터베이스를\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t16\t처음\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t17\t배웠을\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t18\t때\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t19\t황당했다\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t20\tS/W\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t21\t공학이\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t22\t제일\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t23\t재미있는\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t24\t과목이었다\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\t25\t세상에서\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\t26\t포토샵이\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\t27\t싫었다\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\t28\t요즈음에는\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\t29\tML_OPS\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\t30\t나\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\t31\tMetaverse\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\t32\t에\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\t33\t관심이\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\t34\t많다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd9ULXI2JGkw"
      },
      "source": [
        "###  3) 각 문장을 원핫 인코딩한 결과로 출력하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnHXkvGBIGo3",
        "outputId": "006d475d-7cf4-4b38-abf8-567b3581379d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "for sentence in [s1, s2, s3]:\n",
        "    onehot_s = []\n",
        "    tokens = sentence.split()\n",
        "    for token in tokens :\n",
        "        if token2idx.get(token) != None:\n",
        "            vector = np.zeros((1, V))\n",
        "            vector[:, token2idx[token]] = 1\n",
        "            onehot_s.append(vector)\n",
        "        else:\n",
        "            print('UNK')\n",
        "    print(f'{sentence}:') # 원본 문장\n",
        "    print(np.concatenate(onehot_s, axis = 0)) # 원핫 인코딩된 문장\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "나는 처음에 어셈블리 언어를 배웠는데 재미가 너무 없었다:\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "두번째 배운 언어는 코볼이었는데 그나마 조금 나았다:\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n",
            "나는 데이터베이스를 처음 배웠을 때 황당했다:\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jf0ThHcKUTh"
      },
      "source": [
        "## 11.가중치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDOEb1V4ik2M"
      },
      "source": [
        "### 1) Frequency-based Method\n",
        "- 유사도 측정을 하려고 했는데 거리계산이 안되어 등장 횟수에 가중치를 부여하는 방식입니다.\n",
        "- 문장을 표현할 때 각 단어의 원 핫 인코딩 결과를 모두 더해서 표현하는 방법을 이용할 수 있는데 이러한 방식을 BoW(Bag of Words)라고 하기도 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Db2me6imnH"
      },
      "source": [
        "### 2) TF(Term Frequency)\n",
        "- 두번째 방식은 TF(Term Frequency)로 ```{key:token, value:count}```의 형태로 표현할 수도 있습니다. 그러나 이 방식의 문제는 모든 곳에서 자주 등장하는 단어의 가중치가 높아지게 됩니다. 실제로는 중요하지 않고 문장을 구성하는데 필요한 요소들의 가중치가 높아져서 중요한 단어가 되어 버릴 수 있습니다.(예를 들면 a, an ,the...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSYx6vPJiq4d"
      },
      "source": [
        "### 3) IDF(Inverse Document Frequency) \n",
        "- TF를 보정한 방법인 IDF(Inverse Document Frequency) 도입\n",
        "- 현재 문장에서 자주 등장하면 가중치를 높여주고 다른 문장에서도 등장하면 가중치를 낮추는 방식입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S270ig8Viszx"
      },
      "source": [
        "### 4) TF-IDF\n",
        "- TF와 IDF를 같이 사용해서 문장을 벡터화하는 방식\n",
        "- 이 방식은 분류 모델에서 꽤 유용한데 생성 모델에서는 사용할 수 없습니다. 생성 모델은 순서를 고려해야 하는데 TF-IDF는 순서와 상관없이 가중치가 부여되기 때문입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CrpxkVFivDF"
      },
      "source": [
        "### 5) Word Embedding\n",
        "- One Hot Encoding의 결과로는 문장이나 단어의 거리를 계산할 수 없습니다. 왜냐하면, 거리를 계산할 때는 일반적으로 내적(행렬의 곱)을 이용하는데 One hot Encoding으로 만들어진 벡터는 동일한 단어는 거리가 무조건 1이고 나머지 단어끼리는 무조건 0이 되기 때문입니다.\n",
        "- 일정한 크기의 벡터에 단어들을 투영하는 방법입니다.\n",
        "- 데이터가 순차적으로 나온다던가 위치가 같다면 유사한 단어로 취급해서 거리를 가깝게 만들고 아무런 관계가 없다면 멀게 만드는 방식입니다.\n",
        "\n",
        "```\n",
        "나는 자바를 좋아합니다. \n",
        "나는 파이썬을 좋아합니다 \n",
        "나는 PHP를 좋아하지는 않습니다\n",
        "나는 포토샵이 싫습니다.\n",
        "```\n",
        "```나는``` 과 같은 거리에 위치한 ```자바를```, ```파이썬을```,  ```PHP를```, ```포토샵이``` 에 대해 거리를 가깝게 배치합니다.\n",
        "- 이방법을 이용하면 One-Hot Encoding보다 거리 계싼이 편리해지고 새로운 단어가 등장해도 벡터의 크기가 변하지 않습니다.\n",
        "- One - Hot Encoding은 새로운 단어가 등장하면 벡터의 크기를 늘려야 합니다. 이러한 방식을 Word2vec라고 합니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxgoxDplkRTu"
      },
      "source": [
        "## 12.Word2vec\n",
        "- Word2vec방식에는 CBOW와 Skip Gram이 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfZoDvVYkac4"
      },
      "source": [
        "### 1) CBOW\n",
        "- 문맥 토큰을 가지고 타켓을 찾아 냅니다.\n",
        "- 입력층이 여러개이고 출력층이 한개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqw7QKGDkdOj"
      },
      "source": [
        "### 2) Skip Gram\n",
        "- 타겟을 가지고 문맥 토큰을 찾아 냅니다.\n",
        "- 입력층이 한개이고 출력층이 여러개\n",
        "\n",
        "#### =>둘은 방향만 다를 뿐 문맥을 이용한다는 공통점이 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XexNu95_i02V"
      },
      "source": [
        "## 13.Pre-Trained Word Embedding\n",
        "- 위의 방법들은 구글이 제공하는 Sentence piece에서 사용 \n",
        "- https://github.com/google/sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcGlqS6FjfFD"
      },
      "source": [
        "## 14.구글의 Sentencepiece사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIKURu-Jjf72"
      },
      "source": [
        "### 1) sentencepiece라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTw_Uiu3jVXE"
      },
      "source": [
        " !pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMMBU1FFjUqp"
      },
      "source": [
        "### 2) 구글의 Sentencepiece 모델 다운로드\n",
        "\n",
        "    - Linux에서 web에 있는 파일을 다운로드 받을 때 사용하는 명령으로 Linux에서는 !를 붙여서 바로 사용하면 됩니다. \n",
        "    - max에서 수행하고자 하면 wget을 설치한 후 터미널에서 실행해야 합니다. brew도 설치가 되어 있어야 하는데 cpu에 따라 설치 버전이 다릅니다.\n",
        "    - windows에서 수행할 때도 windows용 wget을 설치하고 해야 합니다.[https://eternallybored.org/misc/wget/](https://eternallybored.org/misc/wget/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp5mLIMTjnVe"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WpjVJGLW26Z"
      },
      "source": [
        "### 3) 구글의 sentencepiece를 이용한 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isIVe6AJWRko"
      },
      "source": [
        "# 모델에 데이터를 설정\n",
        "import sentencepiece as spm\n",
        "#input 에 다운로드 받은 파일의 경로를 설정\n",
        "spm.SentencePieceTrainer.train('--input=/Users/adam/botchan.txt --model_prefix=m --vocab_size=2000')\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "#단어 단위로 토큰화\n",
        "print(sp.encode_as_pieces('New York'))\n",
        "print(sp.encode_as_ids('New York'))\n",
        "\n",
        "# decode: id => text\n",
        "print(sp.decode_pieces(['▁New', '▁Y', 'or', 'k']))\n",
        "print(sp.decode_ids([1437, 867, 105, 94]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyxPVn87jLDF"
      },
      "source": [
        "## 14.BERT\n",
        "- https://github.com/google- research/bert\n",
        "- Transformers 라는 개념을 이용\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOh-D3tuacAp"
      },
      "source": [
        "### 1) 버트 사용을 위한 transformer설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDOYN1p2alkO"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_NU4xv5kFhT"
      },
      "source": [
        "### 2) 버트 모델 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-OW3R5XawAQ"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "# 기본 버전 가져오기\n",
        "tokenizer = BertTokenizer.from_pretained('bert-base-uncased')\n",
        "print(len(tokenizer.vocab))\n",
        "print(tokenizer.tokeinze('Sometimes Life is going to hit you in the head with a brick. Dont lose faith'))\n",
        "\n",
        "# 다국어 버전\n",
        "tokenizer = BertTokenizer.from_pretained('bert-base-multilingual-uncased')\n",
        "print(len(tokenizer.vocab))\n",
        "print(tokenizer.tokeinze('아침먹고 땡 집을 나서려는데 화려한 햇살이 나를 감싸네'))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}