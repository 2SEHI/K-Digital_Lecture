# 강화 학습



## 1.강화학습의 의미

강화(Reinforcement) 란 시행 착오를 통해서 학습을 해나가는 방법으로 행동 심리학의 개념을 이용한 것입니다.

### 1) 스키너의 강화 연구

상자 안에 쥐와 지렛대를 넣어서 쥐가 지렛대를 누르면 먹이를 공급하는 장치를 가지고 실험하였습니다. 쥐가 돌아다니다가 우연히 지렛대를 눌러서 먹이가 제공되고 먹이를 먹게 됩니다. 그리고 다음에도 쥐가 돌아다니다가 우연히 지렛대를 눌러서 먹이가 제공되고 먹이를 먹게 됩니다. 이 작업을 몇 번 반복하다 보면 쥐는 지렛대를 누르면 먹이가 제공된다는 사실을 학습하게 되는데 이것을 강화라고 합니다. 

그런데 이 때 쥐는 지렛대와 먹이 사이의 인과 관계는 알지 못하고 자신의 행동과 보상관계만 터득하게 되는데 이를 강화 학습이라고 하며, 강화 학습은 직접 시도하면서 시도한 행동과 그 결과로 나타나는 좋은 보상 사이의 상관관계를 학습하는 것입니다.



### 2) 다른 머신러닝 학습과의 차이점

강화 학습 : 정답이 주어지지 않기 때문에 지도학습이 아니며, 보상이 주어지고 이 보상을 가지고 계속해서 학습해 나가기 때문에 비지도 학습도 아닌 형태이기 때문에 별도의 강화학습으로 분류합니다.

- 지도 학습 : 정답을 알고 있는 데이터를 이용해서 컴퓨터로 학습해서 컴퓨터가 만들어 낸 답과 정답사이의 차이를 줄여나가면서 학습하는 방식으로 Regression과 Classification이 대표적인 지도 학습의 예입니다
- 비지도 학습 : 정답이 없는 데이터를 가지고 비슷한 것끼리 묶는 방식의 학습으로 주성분 분석, 군집이 대표적인 비지도 학습의 예입니다
  - PCA주성분 분석을 하는 이유는 추천 시스템을 만들거나 하나로는 표현할 수 없는 피처의 특징을 설명하기 위해서 입니다
  - 군집은 새로운 분류를 하기 위해서 사용



### 3)  Agent

Agent란 스스로 학습하는 컴퓨터를 말합니다.

사전 지식이 없는 상태에서 환경에 대해 학습을 하는데 자신이 놓인 환경에서 자신의 상태를 인식한 후 행동하고 그러면 Agent에게 보상을 주고 다음 상태를 알려주게 되는데 이 보상을 보고 Agent는 좋은 행동이었는지 잘못된 행동이었는지 보상과 처벌을 통해 알게 되고 보상을 여러번 받다보면 좋은 행동의 패턴을 인식해서 학습을 하게 됩니다.이 때문에 강화학습에서 보상은 Agent에게 중요한 문제입니다

- 보상은 양수 나 음수로 주어지는데 음수의 보상은 처벌이라고 합니다.



### 4) 강화학습의 목적

- 강화학습의 목적은 Agent환경을 탐색하면서 얻는 보상들의 합을 최대화하는 행동 양식이나 정책을 학습하는 것입니다.

- 위에서 설명했듯이 강화학습은 환경을 알지 못해도 학습이 가능합니다.

  - 예를 들어 강화학습을 이용한 대표적인 모델이 알파고인데 알파고는 바둑을 두는 규칙만 알고있는 상태에서 바둑을 두면서 학습을 한 것입니다. 이 점이 지도 비지도 학습과 가장 큰 차이점입니다.

  - 지도학습의 경우 전체 데이터의 정확도가 낮거나 반복 횟수를 설정하면 다시 분류를 하는데 이전에 잘못 분류했던 것은 새로 분류할 때 아무런 의미가 없습니다. 이전에 학습한 내용은 평가 지표를 계산할 때 사용이 되지만 다음 분류를 할 때는 사용이 되지 않습니다.

    - 예를 들면 지도학습은 다음과 같이 1번째 모델에서 A의 분류가 잘 되었지만 2번째엔 잘못된 분류를 하게 되므로 이때 두번째 모델이 더 좋은 모델이라고 할 수 없습니다. 그래서 정확도뿐만 아니라 다른 평가 지표도 고려해야 합니다. 지도학습은 에포크를 늘리면 성능이 좋아지지만 반드시 좋아지는 것이 아닙니다. 
    - 반면 강화 학습은 잘 분류된 A엔 보상을 주고 분류가 안된 B와 C만 가지고 다시 수행하여 보상을 최대로 하는 방향으로 학습합니다. 강화학습으로 분류를 하지는 않지만 기존의 보상은 그대로 두고 처벌에 대한 성능을 개선시켜서 정확도가 33%에서 67%가 되면 모델 성능이 좋아졌다고 할 수 있습니다.

    ![image-20210902150226999](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20210902150226999.png)

  

  - RNN : 다음과 같이 문장이 나열 되어 있다고 할 때 B다음에 나올 글자는 일반적으로 C 또는 F가 올 수 있다고 예측을 합니다. RNN은 현재의 데이터만 가지고 다음 데이터를 예측하는게 아니라 과거 데이터도 포함시켜서 다음 데이터를 예측합니다.

    ```
    A B C
    D B F
    ```

    

  - 알파고는 처음에는 임의로 바둑돌을 두고 그 결과를 확인한 후 바둑을 이겼다면 다음에 같은 상황이 올 때 과거에 두었던 수를 두는 형태로 학습을 합니다. 이런 학습을 무수히 많이 치르면서 만들어집니다.
    - MC(몬테카를로 서치) : 지금까지 두어졌던 모든 바둑의 수를 전부 기억하고 있다가 한 수를 두게되면 그 다음 수들의 모든 경우의 수를 찾아서 승패의 유불리를 판단했습니다. ex) 이세돌과 두었던 알파고
    - 지금까지 두었던 바둑을 기억하지 않고 전부 새로 두면서 학습합니다. ex) 커제와 두었던 알파고
  - 강화학습은 문제 자체에 대해 잘 이해하지 않으면 엉뚱한 결과를 낳게 됩니다.



## 2.순차적 결정 문제

순차적 결정 문제란 한번에 어떤 결정이 이루어지지 않고 여러번에 걸쳐서 문제를 해결하는 문제입니다. 

문제를 해결하는 방법 : 

-  Dynamic Programming : 복잡한 문제를 분할해서 해결
-  진화 알고리즘(Evolutionary Algorithm)



동적 프로그래밍과 진화 알고리즘의 한계 때문에 강화학습이 등장하게되었습니다.
이렇게 순차적으로 행동을 결정하는 문제를 정의할 때 사용하는 방법을 MDP(Markov Decision Process)라고 합니다. 순차적으로 행동을 결정하는 문제를 수학적으로 정의하는 것입니다.



### 1) 구성 요소

- Action(행동) 

Agent가 어떠한 상태에서 취할 수 있는 행동입니다. 게임같은 경우는 상하좌우 움직이는 것것이나 총알을 발사하는 것 등의 동작이 행동입니다. 강화학습에서 처음엔 무작위로 행동을 하게 되고 학습을 하면서 특정한 행동들을 하게될 확률을 높여갑니다. Agent가 행동을 취하면 환경은 Agent에게 보상을 주고 다음 상태를 알려줍니다.

- Reward(보상)

강화학습이 다른 머신러닝 기법과 가장 크게 다른 요소입니다. Agent가 학습할 수 있는 유일한 정보로 Agent가 취한 행동이 좋은 행동인지 아닌지를 판단할 수 있도록 해주는 환경의 일부입니다.

- Policy(정책)

순차적 행동 결정에서 구해야할 답입니다. 순차적 행동 결정 문제를 풀었다고 하면 제일 좋은 정책을 Agent가 학습한 것으로 이러한 정책을 Optimal Policy라고 합니다.



### 2) 방대한 상태를 가진 문제에서의 강화 학습

강화 학습이 나오기 전에도 AI는 장기나 체스는 인간보다 뛰어난 성능을 발휘 했습니다.

바둑은 실제 나올 수 있는 경우의 수가 거의 무한대에 가깝습니다. 보통 10의 361 제곱 정도 된다고 하며 알파고가 인간을 이긴 것은 바둑을 이긴 것이 아니라 아무리 방대하고 경우의 수가 많은 문제라도 컴퓨터가 해결할 수 있다는 것을 보여준 사례입니다.

이러한 비슷한 문제가 로봇의 문제입니다. 구글이 딥마인드를 인수한 가장 큰 이유로, 로봇이 인간처럼 동작하려면 무수히 많은 동작에 대한 보상을 만들어주어야 하는데 이것을 일반적인 머신러닝 기법으로는 해결할 수 없습니다. 

수많은 상태에 대한 정보를 함수와 같은 형태로 제공하는 인공 신경망을 사용해야 합니다. 이런 문제들은 딥러닝과 강화 학습의 조합으로 해결합니다.

현재는 로봇이 레고 블록을 쌓는 정도까지는 도달했으며 자동차도 로봇의 일종으로 자율주행이 어느정도는 가능합니다. 



### 3) BreakOut 게임에서의 학습 방법

1. 문제 정의

   상태(State) : 기본 구성 요소로 게임 화면이 상태가 됩니다

   행동(Action) : 제자리, 왼쪽, 오른쪽으로 이동할 수 있고 발사 동작을 할 수 있습니다.

   ​	행동에 대한 결과로 보상을 주어야 하고 그 결과로 인해 만들어지는 상태를 제공합니다.

   보상(Reward) : 블록이 하나씩 깨질 때마다 +1, 블록이 깨지지 않으면 0, 공을 잃어버리면 -1



2. 학습

   Agent가 게임이나 상황에 대해 모르므로 4가지 동작 중 아무거나 행동을 취하는데 그런 후에 보상을 확인해서 보상이 큰 쪽으로 학습합니다.

   이 때 학습되는 것은 인공 신경망입니다. 인공 신경망으로 입력이 들어오면 신경망은 출력으로 보상의 합계를 주고 이 보상의 합계가 가치가 되서 가장 좋은 가치를 가진 동작을 기억하게 되는데 가장 좋은 가치를 찾아주는 함수를 Q function이라고 합니다. 다른말로는 Q Running이라고 합니다.

   사람과 다른 점은 사람은 화면을 보면 규칙을 대략적으로 미리 알게 되고 그 상태에서 시작하지만 강화 학습은 규칙을 모른 상태에서 시작하기 때문에 초반에 학습이 느립니다. 

   사람과 또 다른점은 사람은 다른 사람의 도움을 받아서 초기 학습이 가능하지만 강화 학습은 이게 안되서 전이 학습이 없습니다.

   강화 학습에는 많은 비용과 자원이 듭니다.



## 3.MDP

마르코프 결정과정이란 뜻으로 순차적으로 행동을 계속 결정해야 하는 문제를 수학적으로 풀어낸 것이 MDP입니다.

### 1) MDP의 구성 요소

상태, 행동, 보상 함수, 상태 변환확률 까지는 순차적 결정 문제와 같으나 감가율이라는 것이 추가적으로 있습니다

#### - 상태

아래와 같은 그리드가 있을 때 🟥네모는 Agent의 현재 위치이고 🔺세모는 패널티이며 🔵동그라미는 목표입니다. Agent가 갈 수 있는 것은 모두 상태가 되고 Agent는 25개의 상태를 모두 탐험하게 됩니다. 시간은 t 상태는 S라고 할 때 Sτ = (1, 3) 이라고 표현합니다.

어떤 집합 안에서 뽑을 때마다 달라질 수 있는 것을 확률 변수(Random Variable)라고 합니다

![image-20210902193928820](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20210902193928820.png)

#### - 행동

Agent가 상태 Sτ에서 가능한 행동의 집합을 A라고 하고, 특정한 행동은 a라고 할 때 Ατ=a라고 표현합니다. 이 때 A는 집합이고 a는 스칼라의 형태입니다. 행동의 집합은 일반적으로 변하지 않습니다.



#### - 보상함수

보상은 에이전트가 학습할 수 있는 유일한 정보로서 환경이 에이전트에게 주는 정보입니다

시간 t일 때 상태가 St = s 이고 그 상태에서 행동 At = a를 했을 경우에 받을 보상에 대한 기댓값(Expectation) 은 E라고 합니다. 이때 보상 함수R 에 대한 수식은 다음과 같습니다
$$
R_{a}^{s}=E[R_{t+1}|S_{t}=s,A_{t}=a]
$$


보상 함수 R을 보상의 기댓값이라고 합니다.기댓값은 일종의 평균으로 나오게 될 숫자에 대한 예상으로 상태 S에서 행동 a를 했을 경우에 받을 것이라 예상되는 확률이라고 보면 됩니다.

수식의 |는 조건문에 대한 표현으로 |를 기준으로 뒤에 나오는 부분들이 현재의 조건을 의미하며 어떤 상태 S에서 행동 a를 할 때마다 받는 보상이 다를 수 있으므로 기댓값의 표현인 E가 붙습니다.

#### - 상태 변환 확률

바람과 같이 예상치 못한 요소가 있으면 에이전트는 앞에 도달하지 못할 수도 있습니다. 상태의 변화에는 이러한 확률적인 요인이 들어 가는데 이를 수치적으로 표현한 것이 상태 변환 확률입니다.

#### - 감가율

시간에 따라서 감가하는 비율을 감가율이라 하며 감가율은 ʏ로 표기하고 0과 1사이의 값으로 보상에 곱해지면 보상이 감소합니다. 수식은 다음과 같이 표기합니다.
$$
\gamma\in[0,1]
$$
만약 현재의 시간 t로부터 시간 k 가 지난 후에 보상을  Rt+k 받는 다고 하면 보상의 가치에 대한 수식은 다음과 같습니다.
$$
\gamma^{k-1}R_{t+k}
$$


### 2) Policy

정책은 각 상태마다 어떤 행동을 해야할지 알려주는데 강화학습의 목적은 일반적인 정책이 아닌 가장 빠르게 도달할 수 있는 최적의 정책을 찾는 것입니다. 그리고 최적의 정책을 알려면 value Function을 사용합니다.



## 4.Value Function

MDP로 정의하면 에이전트는 MDP를 통해 최적 정책을 찾으면 되는데 어떠한 특정한 상태에 에이전트가 있다고 가정해보면 이 에이전트 입장에서 어떤 행동을 하면 좋을 현재 상태에서 앞으로 받을 보상들을 고려해서 선택해야 좋은 선택을 할 수 있는데 아직 받지 않은 많은 보상들을 어떻게 고려할 수 있을지에 대한 문제가 발생하며 이 때 나오는 개념이 Value Function입니다.



### 1) 수식과 같이 보상들을 감가하지 않고 더하면 발생하는 문제

Agent는 언제 받은 보상이든 보상의 합을 단순히 더하기 때문에 1번받은 100의 보상과 5번 받은 20의 보상을 구분할 수 없습니다. 시간이 무한대라고 하면 어느쪽이든 보상의 합은 무한대여서 수치적으로 구분할 수가 없습니다.



### 2) 감가율을 고려한 가치함수

이러한 문제로 Agent는 단순 보상의 합보다 더 정확한 판단을 위해 감가율을 고려합니다. 감가율을 이용해서 앞으로 받을 보상의 현재 가치를 나타내는 것입니다. 수식의 결과값을 반환값 Gt라고 합니다.
$$
G_{t}=R_{t+1} +\gamma R_{t+2}+ \gamma^{2}R_{t+3} \cdots \gamma^{k-1}R_{t+k}
$$


반환값은 Agent가 실제로 환경을 탐험하며 받은 보상의 합으로 Agent가 에피소드가 끝나고 얼마의 보상을 받았는지 정산하는 값이 반환값입니다.

현재의 정보를 토대로 행동하는 것이 나은 경우가 있어서 에피소드가 끝나지 않아도 얼마의 보상을 받을지 예측할 수 있습니다. 이때 어떠한 상태에 있으면 앞으로 얼마의 보상을 받을 것인지에 대한 기댓값이 가치 함수입니다.

가치 함수는 반환값의 기대값으로 수식은 다음과 같습니다.
$$
v(s)=E[G_{t}|S_{t}=s]
$$
가치함수는 Agent가 가지고 있으며 Agent가 현재 갈 수 있는 상태들의 가치를 안다면 그 중에서 가치가 제일 높은 상태를 선택할 수 있습니다.

그리고 기대에 못미쳐도 여러번 반복하다 보면 결국 기대가 정확해지기 때문에 Agent 는 가치함수를 통해 어느 상태가 좋을지 판단합니다.



### 3) 벨만 기대 방정식

MDP에서 가치 함수는 항상 정책을 고려해야 하며 가치 함수와 기댓값의 기호 밑에 정책을 써주면 정책을 고려한 가치 함수가 됩니다. 이를 벨만 기대 방정식이라고 합니다.
$$
v(s)=E[R{_t+1}+\gamma v_{\pi} (S_{t+1})|S_{t}=s]
$$
벨만 기대 방정식은 현재 상태의 가치함수와 다음 상태의 가치 함수 사이의 관계를 말해주는 방정식입니다.



### 4)큐 함수

- 큐함수가 나오게 된 배경 : 마르코프 결정과정의 문제점은 모든 경우의 수를 다 확인해봐야 한다는 것입니다.가치 함수를 통해 다음에 어떤 상태로 가야할지 상태에 대한 행동을 모두 따져봐야하는데 상태에 대한 각 행동에 대해 따로 가치 함수를 만들어서 그 정보를 얻어 올 수 있으면 Agent는 다음 상태의 가치 함수를 따져보디 않아도 어떤 행동을 해야할지 선택할 수 있을 것입니다.
- 어떤 상태에서 어떤 행동이 얼마나 좋은지 알려주는 함수를 행동 가치 함수라고 하고 간단하게 큐 함수(Q Function) 라고 합니다.

- 몬테칼로 서치는 출발지점에서 모든 경우를 찾는 것입니다. 순차적으로 진행하고 되돌아 갈 수 없다면 처음부터의 최적이 아니라 현재의 최적을 찾아야 합니다. 현 위치에서의 최적을 찾는 것이 큐 함수입니다. 
- 큐 함수는 상태, 행동이라는 두가지 변수를 가집니다.

- 큐 함수를 벨만 기대 방정식의 형태로 나타낼 수 있습니다.

## 5.벨만 기대 방정식

- Bellman Equation
- 어떤 상태의 가치 함수는 Agent가 어떤 상태로 갈 경우에 앞으로 받을 보상의 합에 대한 기댓값으로 현재 상태의 가치 함수와 다음 상태의 가치 함수 사이의 관계를 식으로 나타낸 것입니다.
- 앞으로 받을 모든 보상에 대해 고려해야하는데 물리적인 한계로 불가능합니다. 이 때문에 방정식 하나를 한번에 풀어내기 보다는 반복계산을 통해 참 값을 알아나가는데 이 과정은 다이내밍 프로그래밍과도 관련이 있습니다.

## 6.벨만 최적 방정식

벨만 기대 방정식은 처음에 가치 함수의 들이 의미가 없는 값으로 초기화되며 초깃값으로부터 시작해서 수식의 벨만 기대 방정식으로 반복적으로 계산한다고 가정해보면 이 계산을 반복하다 보면 



여기부터 다시정리하기



- 벨만 기대 방정식

- - 벨만 기대 방정식 : 현재에서 가장 좋은 것을 찾자
  - 벨만 최적 방정식 : 전체에서 가장 좋은 것을 찾기
- 다이너믹 프로그래밍
  - 벨만 최적 방정식과 정책 기반 이터레이션은 출발점에서 도착점 까지의 최적의 경로를 찾는 방식
  - 정책기반 이터레이션과 가치 기반의 이터레이션
    - 최적기반이나 정책기반은 출발점에서 도착점까지의 최적의 경로를 찾아내는 방식입니다. 골이 명확한 경우(미로 찾기나 격투 게임)는 찾을 수 있으나 , 슈팅 게임에서는 사용할 수 없습니다. 그래서 가치 기반이나 정책 기반은 출발점에서 도착점까지의 최적 경로를 찾는 것이 아니라 현재 상태에서 최적의 경루를 찾는 것입니다. 대다수의 문제는 이 방법으로 해결합니다.



## 다이너믹 프로그래밍

## 1) 순차적 행동 결정 문제를 해결하는 방식

순차적 행동 문제를 MDP로 전환해야 합니다.(수학적 수식으로 전환)

가치 함수를 벨만 방정식으로 반복적으로 계산하여 최적 가치 함수와 최적의 정책을 찾아냅니다.

알고리즘 문제를 해결 하고자 할 때 

- 입력이 무엇인지 출력이 무엇인지 그리고 제약 조건이 있는지  문제의 의도를 정확하게 파악해야 합니다. 

- 복잡한 문제 해결은 분할과 정복, 다이나믹 프로그래밍을 이용해야 합니다. 큰 문제 안에 작은 문제들이 중첩된 경우 전체 큰 문제를 작은 문제로 쪼개서 풀어야 합니다.여러가지 기능을 구현해야 하는 경우 각각의 기능을 함수로 만들고 모여서 하나의 기능을 해야 하면 클래스로 합쳐주면 됩니다.



### 2) Dynamic Programming

- 큰 문제 안에 작은 문제들이 중첩된 경우 전체 큰 문제를 작은 문제로 쪼개서 해결하는 것
- 작은 문제를 하나씩 시간에 따라 별도의 프로세스로 풀어나가기 때문에 다이나믹 프로그래밍이라고 합니다. 이렇게 하게 되면 계산량을 줄일 수 있어서 복잡한 문제를 쉽고 빠르게 해결할 수 있습니다.



## 5. 정책 이터레이션과 가치 이터레이션

- 정책 이터레이션과 전체경로에서 최적의 해결책을 찾는 것
- 가치 이터레이션은 현재 위치에서 최적의 해결책을 찾는 것



