{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch(RNN-LSTM, GRU 다음 문장 예측)ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1a7J_xqyFmt9VvCwQlVHO8UNIEYnTgyn0",
      "authorship_tag": "ABX9TyN7l8uaGGCpwEG81nRLaEy6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2SEHI/K-Digital_Lecture/blob/main/0820_pyTorch(LSTM%2C%20GRU%2C%20BERT)/pyTorch(RNN_LSTM%2C_GRU_%EB%8B%A4%EC%9D%8C_%EB%AC%B8%EC%9E%A5_%EC%98%88%EC%B8%A1)ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tqArPoc3QEH"
      },
      "source": [
        "# RNN - LSTM, GRU, Attention\n",
        "\n",
        "Transformer : 자연어 처리나 전이 학습\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBfLSife8Jui"
      },
      "source": [
        "## 1. GRU(Gated Recurrent Unit)란?\n",
        "- LSTM보다 간단한 구조를 가지고 LSTM과 유사한 성능을 만든 RNN의 변형입니다\n",
        "- 셀 상태가 별도로 구현하지 않고 셀 상태와 은닉 상태를 하나로 합쳐서 구현하는 모델입니다.\n",
        "\n",
        "- 망각 게이트(Reset gate)와 업데이트 게이트(Update gate)의 개념을 이용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ApA5oEk8K5L"
      },
      "source": [
        "\n",
        "## 2. LSTM(Long - Term Memory)이란?\n",
        "- 오랫동안 기억을 하기 위해서 Cell State를 추가한 것\n",
        "- 기본적인 RNN(현재의 입력 값과 이전 시간의 은닉층 값의 조합으로 새로운 값을 만들어 냄)에 비해 하나의 상태가 더 생긴 것으로 LSTM은 입력 값과 이전 시간의 은닉층 값과 셀 상태의 조합으로 새로운 값을 만들어 냄\n",
        "\n",
        "- LSTM에서 아래 과정을 거치면 기존의 RNN보다 더 좋은 성능을 가집니다.\n",
        "    - 셀 상태는 장기기억부분으로 기존의 정보을 얼마나 남겨놓을지 \n",
        "    - 망각 게이트 : 시그모이드 함수를 적용합니다. 셀 상태 값을 얼마나 잊어버릴 것인지 \n",
        "    - 입력 게이트 : 시그모이드 함수를 적용. 하이퍼볼릭 탄젠트를 통과시켜서 추가합니다.\n",
        "    - 은닉 상태 업데이터\n",
        "    - 망각 게이트(Forget gate)와 입력 게이트(input gate)의 개념을 이용\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6L6Ph1d8GEq"
      },
      "source": [
        "# 문장 생성 구현을 RNN, GRU, LSTM으로 구현\n",
        "- 입력받은 문장의 다음 문장을 예측하는 기능을 구현\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LqX_p61Fl77"
      },
      "source": [
        "## 공통 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic0Y3E167ZzC"
      },
      "source": [
        "### 1) 사용할 데이터\n",
        "- 아래 코드를 실행하여 셰익스피어 소설 데이터를 가져옵니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-7NwsSp4kcV",
        "outputId": "9deda56d-2254-46d6-864d-c4e615f6c8f7"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-20 00:17:56--  https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-08-20 00:17:56 (20.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjKx37pH7RcY"
      },
      "source": [
        "### 2) 필요한 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsFeojJr7efK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f174f7fd-c9a5-481f-f16b-f8412219906a"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 92 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 102 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 112 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 122 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 133 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 143 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 153 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 163 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 174 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 184 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 194 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 204 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 215 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 225 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 235 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 241 kB 4.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdIar9hG7oHb"
      },
      "source": [
        "### 3) 필요한 라이브러리 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8gnPcWh7np6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# 문자열 처리 관련 라이브러리\n",
        "import string\n",
        "import re\n",
        "import unidecode\n",
        "# 랜덤 모듈\n",
        "import random\n",
        "# 시간, 수학 모듈\n",
        "import time, math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rjvOmdX8Sx6"
      },
      "source": [
        "### 4) 하이퍼 파라미터 설정\n",
        "- 보통 chunk라는 단어가 들어가면 많은 양의 데이터를 분할해서 읽는다는 의미입니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1fP-rA18XRJ"
      },
      "source": [
        "num_epochs = 2000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "\n",
        "# 많은 양의 데이터 분할\n",
        "chunk_len = 200\n",
        "\n",
        "hidden_size = 100\n",
        "batch_size = 1\n",
        "num_layers = 1\n",
        "embedding_size = 70\n",
        "lr = 0.002"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNhKQYW89-Xa"
      },
      "source": [
        "### 5) import 했던 string에서 출력가능한 문자들을 다 불러옴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5PfaSmC92Tz",
        "outputId": "70d64e84-e4d8-4b10-dd4d-5df32c400973"
      },
      "source": [
        "# import 했던 string에서 출력가능한 문자들을 다 불러옴\n",
        "all_characters = string.printable\n",
        "\n",
        "# 출력가능한 문자들의 개수를 저장함\n",
        "n_characters = len(all_characters)\n",
        "print(all_characters)\n",
        "print('num_chars = ', n_characters)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
            "\r\u000b\f\n",
            "num_chars =  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USqc8mh1-mMT"
      },
      "source": [
        "### 6) 샘플 데이터 읽어오기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8AcZyLK-tVk",
        "outputId": "cef76197-f7a0-429c-e87c-b1f798f36ccb"
      },
      "source": [
        "file = unidecode.unidecode(open('/content/input.txt').read())\n",
        "file_len = len(file)\n",
        "print(file_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NTkc40q_DA4"
      },
      "source": [
        "### 7) 샘플을 랜덤하게 불러오는 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe5gu07Z_NMB",
        "outputId": "38d0bc79-7014-4de2-d2d6-d2a642005a75"
      },
      "source": [
        "def random_chunk():\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    # print(start_index)\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enhead, at twelve year old,\n",
            "I bade her come. What, lamb! what, ladybird!\n",
            "God forbid! Where's this girl? What, Juliet!\n",
            "\n",
            "JULIET:\n",
            "How now! who calls?\n",
            "\n",
            "Nurse:\n",
            "Your mother.\n",
            "\n",
            "JULIET:\n",
            "Madam, I am here.\n",
            "What i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8hYUljLAOma"
      },
      "source": [
        "### 8) 문자열을 숫자의 list로 변경해주는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIB0T2aM_NGi",
        "outputId": "04c0865f-0f66-45b0-a61f-77ffb9560954"
      },
      "source": [
        "def char_tensor(string):\n",
        "    # string길이 만큼 0으로 채워진 배열 생성 \n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    \n",
        "    for c in range(len(string)):\n",
        "        # string의 문자에 대해 all_characters와 매칭되는 인덱스 번호를 tensor배열에 저장\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return tensor\n",
        "\n",
        "print(char_tensor(\"randomstring\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([27, 10, 23, 13, 24, 22, 28, 29, 27, 18, 23, 16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU3vS4MrBAV7"
      },
      "source": [
        "### 9) 랜덤한 텍스트를 가져와서 입력과 목표 값으로 변환해주는 함수\n",
        "- 다음 글자를 예측처리를 위해 입력할 땐 마지막 글자를 제외하고 \n",
        "- 출력할 땐 첫글자를 제외합니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bJiSfE-_NDz"
      },
      "source": [
        "def random_training_set():\n",
        "    # 샘플 문장 가져오기\n",
        "    chunk = random_chunk()\n",
        "    # print(chunk)\n",
        "    # print(chunk[:-1])\n",
        "    # 샘플 문장을 숫자 리스트로 변환\n",
        "\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqHpCoXsFUAj"
      },
      "source": [
        "## Basic RNN 구현\n",
        "\n",
        "- RNN 모델의 기본 틀\n",
        "\n",
        "    ```python\n",
        "    class RNN(nn.Module) : \n",
        "        def __init__(super):\n",
        "\n",
        "        def forward(self, input, hidden):\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPri65hX_M7q"
      },
      "source": [
        "### 1) RNN 모델 설계\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddKB6XRj7QFG"
      },
      "source": [
        "class RNN(nn.Module) : \n",
        "\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        # 인스턴스 변수 생성\n",
        "        # 입력의 크기 - 임의 설정 불가능\n",
        "        self.input_size = input_size \n",
        "        # 임베딩사이즈의 크기 - 임의로 정하는 것이 가능\n",
        "        self.embedding_size = embedding_size \n",
        "        # 은닉층에서의 뉴런의 개수 - 임의로 정하는 것이 가능\n",
        "        self.hidden_size = hidden_size\n",
        "        # 출력의 개수 - 입력의 크기와 같다\n",
        "        self.output_size = output_size\n",
        "        # 은닉층의 개수 - 임의로 지정이 가능\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # 입력, 은닉, 출력층을 생성\n",
        "\n",
        "        # 입력층\n",
        "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        # 은닉층 - 위에 출력(self.embedding_size)을 입력으로 받음\n",
        "        self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers)\n",
        "        # 출력층\n",
        "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    # 순방향 훈련할 때 호출되는 함수\n",
        "    # nn.Module의 메소드를 overriding\n",
        "    # RNN과 GRU는 입력과 hidden상태의 조합으로 출력을 생성\n",
        "    def forward(self, input, hidden):\n",
        "        \n",
        "        # 입력층을 사용하는 부분\n",
        "        # view가 Flatten역할\n",
        "        out = self.encoder(input.view(1, -1))\n",
        "\n",
        "        # 은닉층에서는 입력과 이전 가중치를 사용해서 훈련하는데\n",
        "        # 이전 가중치를 사용해야 하므로 out, hidden의 변수명을 일치시켜야 함\n",
        "        out, hidden = self.rnn(out, hidden)\n",
        "\n",
        "        # 출력층 -  hidden은 decoder로 넘기지 않고 out만 넘겨줌\n",
        "        out = self.decoder(out.view(batch_size, -1))\n",
        "        # 출력과 이전 가중치를 리턴해서 다시 다음 층으로 전달\n",
        "        return out, hidden\n",
        "\n",
        "    # 호출될 때 이전 가중치인 hidden값을 0으로 초기화\n",
        "    # 사용자 정의 메소드\n",
        "    def init_hidden(self):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        return hidden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znP1wMCyL9UN"
      },
      "source": [
        "### 2) 모델 생성\n",
        "함수를 호출할 때 ```RNN.init_hidden(model)``` 또는 ```model.init_hidden()```로 호출 가능합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4NTdU1zL_zf"
      },
      "source": [
        "model = RNN(input_size=n_characters, embedding_size=embedding_size, \n",
        "            hidden_size=hidden_size, output_size=n_characters,\n",
        "            num_layers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igOx1fItNq4b"
      },
      "source": [
        "### 3) 모델 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0OO5BZRNsEv",
        "outputId": "2cf9ad05-cf6b-4a74-93a3-4ca2ec7adee1"
      },
      "source": [
        "inp = char_tensor('A')\n",
        "print(inp)\n",
        "\n",
        "# \n",
        "hidden = model.init_hidden()\n",
        "print(hidden.size())\n",
        "\n",
        "# forward함수가 실행됨\n",
        "out, hidden = model(inp, hidden)\n",
        "print(out.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([36])\n",
            "torch.Size([2, 1, 100])\n",
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qc4K6fDN4bM"
      },
      "source": [
        "### 4) 하나의 문자를 대입하면 200짜리 텍스트를 생성해주는 함수\n",
        "- numpy.exp() 함수는 밑이 자연상수 e인 지수함수(e^x)로 변환\n",
        "- torch.multinomial 함수는 2개의 인자를 받는데, 첫번째 인자는 확률로 해석될 수 있는 텐서이고 두번째는 샘플링할 개수이다. 첫번째 인자는 확률로 해석할 수 있지만, 정규화될 필요는 없다. 여기서 정규화란 더해서 1이 되어야 한다는 의미이다. 결과에서 보면 알 수 있듯이 샘플링된 값의 인덱스 값이 반환된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVefPQVHTIqK"
      },
      "source": [
        "def test() :\n",
        "    start_str = 'b'\n",
        "    inp = char_tensor(start_str)\n",
        "    hidden = model.init_hidden()\n",
        "    x = inp\n",
        "\n",
        "    # 중간결과는 출력문에서 안나오므로 출력해서 확인\n",
        "    print(start_str, end='')\n",
        "\n",
        "    # b입력 x, 상태 - x, 상태가 입력되서 x\n",
        "    for i in range(200):\n",
        "        output, hidden = model(x, hidden)\n",
        "        # tensor의 형태이므로 펼쳐냄\n",
        "        output_dist = output.data.view(-1).div(0.8).exp()\n",
        "\n",
        "        # 다음 글자를 꺼낼 때 max값을 꺼내면 매번 같은 값이 나오므로 \n",
        "        # 랜덤하게 뽑아내기 위해서 multinomial을 적용\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # 실제 글자 가져오기\n",
        "        predicated_char = all_characters[top_i]\n",
        "        print(predicated_char, end='')\n",
        "        x = char_tensor(predicated_char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pdtNHUEVNeq"
      },
      "source": [
        "### 5) 모델 훈련\n",
        "- 훈련 횟수가 2000번이므로 약간의 시간이 필요한데 시간을 줄이고자 하면 num_epochs의 값을 수정하면 됩니다\n",
        "- 2000번 정도 훈련을 하면 그래도 처음 나온 문장들보다는 제대로 된 문장들이 나올 가능성이 높습니다.\n",
        "- 실제로는 더 많은 양의 데이터가 필요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNNSdawBVQKc",
        "outputId": "6da741d3-7fee-457c-f532-926606f9069b"
      },
      "source": [
        "# 최적화함수와 손실 함수 설정\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# 손실함수 : 크로스 엔트로피 \n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    # 랜덤한 문자열 가져오기\n",
        "    # inp:입력, label:목표\n",
        "    inp, label = random_training_set()\n",
        "    \n",
        "    # 은닉의 개수 초기화\n",
        "    hidden = model.init_hidden()\n",
        "    \n",
        "    # 손실 함수 생성\n",
        "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
        "\n",
        "    # 미분값을 초기화\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for j in range(chunk_len -1 ):\n",
        "        x = inp[j]\n",
        "        # squeeze는 차원이 1인 차원을 제거\n",
        "        # unsqueeze는 차원을 늘려줍니다.\n",
        "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
        "        y, hidden = model(x, hidden)\n",
        "\n",
        "        # 손실은 y(예측값)와 y_(실제 레이블 값) 과의 차이로 계산\n",
        "        loss += loss_func(y, y_)\n",
        "\n",
        "    # 역전파\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(loss/chunk_len, '\\n')\n",
        "        test()\n",
        "        print('\\n', '=' * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.7816], grad_fn=<DivBackward0>) \n",
            "\n",
            "bar of not his to tall of the dracted and not in that my o'll lead to blive sine.\n",
            "Where our theseld they shery his conter'd I all net thear yound her batient have stouse your come the lide the stay's s\n",
            " ====================================================================================================\n",
            "tensor([1.8974], grad_fn=<DivBackward0>) \n",
            "\n",
            "but that homiof, git, reather?\n",
            "\n",
            "Firth and the dauch at, I have made me, ot of for thou fortuired at that of the come to they our nown, sir, well to beat titterd the prodnt 'tis they rel, so the byour a\n",
            " ====================================================================================================\n",
            "tensor([1.5537], grad_fn=<DivBackward0>) \n",
            "\n",
            "besent!\n",
            "\n",
            "QUEEN ELIOLENH:\n",
            "St\n",
            "'twhat my liege, in I cannot will are good that done,\n",
            "And man this we corfong\n",
            "That hore!\n",
            "\n",
            "CETRUCHIO:\n",
            "If thou reint my lord\n",
            "Filery care let the coufight it be raif'd mork the\n",
            " ====================================================================================================\n",
            "tensor([1.8980], grad_fn=<DivBackward0>) \n",
            "\n",
            "but stass on I wheper the his farewe fiVlo be nighter, my barishands, is Myselt do it dood me to deable of here to to hear comes well a king gake your lord,\n",
            "And in newel that I will not that eurse\n",
            "Come\n",
            " ====================================================================================================\n",
            "tensor([1.6860], grad_fn=<DivBackward0>) \n",
            "\n",
            "bet of to my parrow's matities of so stay in the rease? and the sunce the peep of in to marjain thanks thou distell rave to plick take it mine your lovenide tere the sut such be me and you, or land of \n",
            " ====================================================================================================\n",
            "tensor([1.6915], grad_fn=<DivBackward0>) \n",
            "\n",
            "be\n",
            "I dose a known of this is have world most speak'd he's my lords, Cay there did us now very hear thy and come's save the rood, the sporded;\n",
            "Thing art thee whery not shade stother,\n",
            "As haster that more\n",
            " ====================================================================================================\n",
            "tensor([1.8466], grad_fn=<DivBackward0>) \n",
            "\n",
            "be let the with me wear the poice:\n",
            "Thing desice.\n",
            "\n",
            "BSTAUSTA:\n",
            "As love I loves spore on on me of upon\n",
            "Grevors to happerion.\n",
            "\n",
            "PETRUCHARINA:\n",
            "O, what on answroughollait ong by a the lit, I sines is the death\n",
            " ====================================================================================================\n",
            "tensor([1.8320], grad_fn=<DivBackward0>) \n",
            "\n",
            "by, but, the princess,\n",
            "And daliou to thou doss is the holacce.\n",
            "\n",
            "KING RICHARD II:\n",
            "The brother my brother her.\n",
            "\n",
            "ToM's consen it not to ba bitits of the quick me to unblandends door to he the that my let \n",
            " ====================================================================================================\n",
            "tensor([1.5605], grad_fn=<DivBackward0>) \n",
            "\n",
            "be stay,\n",
            "I scertain.\n",
            "\n",
            "LEONTES:\n",
            "By tend friender of that of for id my lord, the cack ands stand at the Kisent saints be and make of stay:\n",
            "But at your man my lorde\n",
            "And pried, Fear his grant eyes off grun\n",
            " ====================================================================================================\n",
            "tensor([1.6872], grad_fn=<DivBackward0>) \n",
            "\n",
            "bere a culling craked for the hould the are to be tollidstil; and for thou worstian:\n",
            "Then he makes\n",
            "For Can, to dissial enemies!\n",
            "How prise, lad of wife mower to stride,\n",
            "Man that syought dost death,\n",
            "I kn\n",
            " ====================================================================================================\n",
            "tensor([1.7398], grad_fn=<DivBackward0>) \n",
            "\n",
            "be'd of your be the spirend and a duent porth:\n",
            "I fust Henry be'then in like a worn I beer that streast,\n",
            "I shine,\n",
            "A feand:\n",
            "For his he the roomech your such the feed shall endly year of than be stame a s\n",
            " ====================================================================================================\n",
            "tensor([1.9985], grad_fn=<DivBackward0>) \n",
            "\n",
            "by the wear\n",
            "The world, sweet it my so chold, and Taster,\n",
            "The would thrunce\n",
            "The done of yet with they forder forthice strucesty in the king; to all, 'twry thou might whine of Jove being eld law hongers \n",
            " ====================================================================================================\n",
            "tensor([1.7783], grad_fn=<DivBackward0>) \n",
            "\n",
            "beak you protely sparcio, conder, be withe lick to extice you dalks alls savents cresence of tear whom canster Rike dear for thee you bride to gleagled on are everalling as with a tilly, sir.\n",
            "\n",
            "GLOUCEST\n",
            " ====================================================================================================\n",
            "tensor([1.4523], grad_fn=<DivBackward0>) \n",
            "\n",
            "be would so couring so is not her have let a votis mear that of it in there we would that and my lord.\n",
            "\n",
            "DUCK:\n",
            "My recess what that hear's rose, and let what have and in the deads up sounds the sed thou \n",
            " ====================================================================================================\n",
            "tensor([1.8060], grad_fn=<DivBackward0>) \n",
            "\n",
            "be shall to the mind it for not a did Lorde of the housated\n",
            "The liat of exess,\n",
            "And gut ill out it she love the did out a say, wish did frough that for the good, by mans their right and nor the him we p\n",
            " ====================================================================================================\n",
            "tensor([1.2097], grad_fn=<DivBackward0>) \n",
            "\n",
            "brown resh when the rained do a stand is whome lay come of love 'tis they condencess she is sle resesing a more;\n",
            "With of great,\n",
            "The world stired concuried be this name to hand\n",
            "Wome let mistlust; the wa\n",
            " ====================================================================================================\n",
            "tensor([1.7218], grad_fn=<DivBackward0>) \n",
            "\n",
            "be it a speak, in her fair not firencery.\n",
            "\n",
            "KING EDWAS:\n",
            "I am were for they fun the sung me was despeed, I do creade a traithout where's that bittere the drave allable:\n",
            "Fnorract not is me.\n",
            "Feens my lorde\n",
            " ====================================================================================================\n",
            "tensor([1.5883], grad_fn=<DivBackward0>) \n",
            "\n",
            "by my dessle,\n",
            "For he ale dists.\n",
            "I was foor to be be not great she so, and love of must lidg will.\n",
            "\n",
            "MENENIUS:\n",
            "Hath of Clidiser at to badous not requit a is our cause of an to my for me be desedve mind C\n",
            " ====================================================================================================\n",
            "tensor([1.7531], grad_fn=<DivBackward0>) \n",
            "\n",
            "beat the let at not to me, for and all such new the cousom'd underor for in a tiny he rest fell or.\n",
            "\n",
            "LADY JFRRONDERSINIUS:\n",
            "On his not the word care, but that she space is to tear hath entress.\n",
            "\n",
            "JULIET:\n",
            " ====================================================================================================\n",
            "tensor([1.6918], grad_fn=<DivBackward0>) \n",
            "\n",
            "bust on toumpod--\n",
            "\n",
            "BRUCUNEN OF YORK:\n",
            "Soul the balk him brofiien.\n",
            "\n",
            "CORIOLAND:\n",
            "My looks and my lords! stapt,\n",
            "And and your woy woe\n",
            "Assy to save prest oth there confurexty, or ever foot, take that sook goo\n",
            " ====================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAzXBoPQYg4c"
      },
      "source": [
        "## RNN모델을 GRU모델로 변경\n",
        "- RNN과 GRU는 은닉 상태만 가지므로 RNN클래스를 이용해서 생성했던 층을 GRU로만 변경해주면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwRADRbaY-r_"
      },
      "source": [
        "### 1) GRU 모델 설계\n",
        "- GRU은 RNN과 역할은 다르지만 구조적으로 차이가 없습니다.\n",
        "- 수정 부분 -  ```__init__```함수만 수정하면 됨\n",
        "    1. ```super(RNN, self).__init__()``` <br>\n",
        "        --> ```super(GRU, self).__init__()```\n",
        "    2.  ```self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers)```<br>\n",
        "        -->  ```self.rnn = nn.GRU(self.embedding_size, self.hidden_size, self.num_layers)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4M3E-rfYgxV"
      },
      "source": [
        "class GRU(nn.Module) : \n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
        "        super(GRU, self).__init__()\n",
        "        # 인스턴스 변수 생성\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # 입력, 은닉, 출력층을 생성\n",
        "\n",
        "        # 출력: self.embedding_size\n",
        "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        # 위에 출력(self.embedding_size)을 입력으로 받음\n",
        "        # self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers)\n",
        "        self.rnn = nn.GRU(self.embedding_size, self.hidden_size, self.num_layers)\n",
        "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    # 순방향 훈련할 때 호출되는 함수\n",
        "    # nn.Module의 메소드를 overriding\n",
        "    def forward(self, input, hidden):\n",
        "        # view가 Flatten역할\n",
        "        out = self.encoder(input.view(1, -1))\n",
        "        out, hidden = self.rnn(out, hidden)\n",
        "        # hidden은 decoder로 넘기지 않고 out만 넘겨줌\n",
        "        out = self.decoder(out.view(batch_size, -1))\n",
        "        return out, hidden\n",
        "\n",
        "    #hidden값을 초기화해주는 함수\n",
        "    # 사용자 정의 메소드\n",
        "    def init_hidden(self):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        return hidden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7G7Eo_NZYzp"
      },
      "source": [
        "### 2) 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ur69jMvZYzv"
      },
      "source": [
        "model = GRU(input_size=n_characters, embedding_size=embedding_size, \n",
        "            hidden_size=hidden_size, output_size=n_characters,\n",
        "            num_layers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqy3CzBQZfcL"
      },
      "source": [
        "### 3) 모델 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmqMbilmZfcL",
        "outputId": "23899603-6679-4081-98b0-53222566b1c0"
      },
      "source": [
        "inp = char_tensor('A')\n",
        "print(inp)\n",
        "\n",
        "# \n",
        "hidden = model.init_hidden()\n",
        "print(hidden.size())\n",
        "\n",
        "# forward함수가 실행됨\n",
        "out, hidden = model(inp, hidden)\n",
        "print(out.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([36])\n",
            "torch.Size([2, 1, 100])\n",
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJWqar3NZfcM"
      },
      "source": [
        "### 4) 모델 훈련\n",
        "- 훈련 횟수가 2000번이므로 약간의 시간이 필요한데 시간을 줄이고자 하면 num_epochs의 값을 수정하면 됩니다\n",
        "- 2000번 정도 훈련을 하면 그래도 처음 나온 문장들보다는 제대로 된 문장들이 나올 가능성이 높습니다.\n",
        "- 실제로는 더 많은 양의 데이터가 필요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rk9XfHIZfcM",
        "outputId": "20ecb55e-706c-4ad0-ec52-9edc085a37b8"
      },
      "source": [
        "# 최적화함수와 손실 함수 설정\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# 손실함수 : 크로스 엔트로피 \n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    # 랜덤한 문자열 가져오기\n",
        "    # inp:입력, label:목표\n",
        "    inp, label = random_training_set()\n",
        "    \n",
        "    # 은닉의 개수 초기화\n",
        "    hidden = model.init_hidden()\n",
        "    \n",
        "    # 손실 함수 생성\n",
        "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
        "\n",
        "    # 미분값을 초기화\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for j in range(chunk_len -1 ):\n",
        "        x = inp[j]\n",
        "        # squeeze는 차원이 1인 차원을 제거\n",
        "        # unsqueeze는 차원을 늘려줍니다.\n",
        "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
        "        y, hidden = model(x, hidden)\n",
        "\n",
        "        # 손실은 y(예측값)와 y_(실제 레이블 값) 과의 차이로 계산\n",
        "        loss += loss_func(y, y_)\n",
        "\n",
        "    # 역전파\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(loss/chunk_len, '\\n')\n",
        "        test()\n",
        "        print('\\n', '=' * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4.5865], grad_fn=<DivBackward0>) \n",
            "\n",
            "-<B7hsr+H,cn /YTvAC6 w|>@m]o\\Min-$c\tu(HrA;X{4nj2%.*UpaDv;`M&arrbga=\n",
            "{qfm8G ^TTjh|yfl\n",
            " ====================================================================================================\n",
            "tensor([2.4307], grad_fn=<DivBackward0>) \n",
            "\n",
            "bsre ths be be mal, sond fo out ous mog\n",
            "?I I,\n",
            "KNIB, he mes yre sis hafe whet the nout, ho, the ncgou shor theet te?\n",
            "EyRT.\n",
            "\n",
            "es me te !eat.\n",
            "\n",
            "BO INULUIEAIRDWLGEFW I reat to ule me oresit he yor meso lce o\n",
            " ====================================================================================================\n",
            "tensor([2.2206], grad_fn=<DivBackward0>) \n",
            "\n",
            "bor for cand to for cears olh,\n",
            "Whean\n",
            " were had lod ereld to oof home wis hes fircof be ceus whofir bear hat mat oct neaad cathe wers thoud berult to thind faleat sey il pafins kine to wanvor she, noulr\n",
            " ====================================================================================================\n",
            "tensor([2.2845], grad_fn=<DivBackward0>) \n",
            "\n",
            "b)se loed,\n",
            "To\n",
            "Dith Lor prou' hit nours, my kee ney, Callo so the coull mard I burter so the you ret not of on no'd head deet wofretrelf smor, of thad sean so noy terefon hin; shere, gace kine her, of f\n",
            " ====================================================================================================\n",
            "tensor([1.9472], grad_fn=<DivBackward0>) \n",
            "\n",
            "bemte in unthen.\n",
            "\n",
            "KIN I ARHENY HINAR:\n",
            "I ull leive me wod be are tho man in thee frose hould.\n",
            "\n",
            "QUENRHIO:\n",
            "And an mate low unt ik hen wat to a I not me grour ung with ence.\n",
            "\n",
            "PINUF HRER:\n",
            "And anbe the I loa\n",
            " ====================================================================================================\n",
            "tensor([2.0133], grad_fn=<DivBackward0>) \n",
            "\n",
            "bary, not grimat wang that hem is blide prove of the\n",
            "so spead, the now not hez if aftlespuw the cat this bace man and bard best it of my lorts I well I gair and lok the fator man:\n",
            "You your hen that you\n",
            " ====================================================================================================\n",
            "tensor([1.9176], grad_fn=<DivBackward0>) \n",
            "\n",
            "beronoueten:\n",
            "Where id the dantss everd,\n",
            "Thy but ower,\n",
            "Thy cond well ward hoge as\n",
            "Were, hown that feer will are, pithich the freet on thee\n",
            "But Risores,\n",
            "As ch'there:\n",
            "Come I lan Gollateath I done grear yo\n",
            " ====================================================================================================\n",
            "tensor([2.0099], grad_fn=<DivBackward0>) \n",
            "\n",
            "bere?\n",
            "\n",
            "BOMLOUUL:\n",
            "There me that meser is so which well reange hand.\n",
            "\n",
            "HULERT:\n",
            "I well I most wat is the com Gair me it livigs and dish\n",
            "Then gromhours that of ather\n",
            "Buses\n",
            "Yout I do are deatiney\n",
            "he dement t\n",
            " ====================================================================================================\n",
            "tensor([1.9649], grad_fn=<DivBackward0>) \n",
            "\n",
            "bece rose.\n",
            "\n",
            "LARIO:\n",
            "And is the broveftion exentle soer heaver, dowray, my proved for will hear to look,\n",
            "A stard furdesgent that no bries,\n",
            "Your I say say plighter, coment haight shall then my more not ri\n",
            " ====================================================================================================\n",
            "tensor([1.8739], grad_fn=<DivBackward0>) \n",
            "\n",
            "btle foly fall.\n",
            "\n",
            "MARD IIS:\n",
            "But man: me pragite thalt the world.\n",
            "\n",
            "KINCES:\n",
            "And give him do your crees tleed too pelpess my coom cond lifed the wor, your my plawing?\n",
            "\n",
            "YORCCICHAR:\n",
            "I bet string shall compla\n",
            " ====================================================================================================\n",
            "tensor([1.8450], grad_fn=<DivBackward0>) \n",
            "\n",
            "barves hearmefores that to till his did.\n",
            "\n",
            "KENTINCENHA:\n",
            "Gor is me pother to to who lord and so a the to thils his.\n",
            "\n",
            "VENG RICDIA:\n",
            "We shees mesself that is meace reaterrvife pillroused that viom\n",
            "This from\n",
            " ====================================================================================================\n",
            "tensor([1.9772], grad_fn=<DivBackward0>) \n",
            "\n",
            "bion is all time yannot.\n",
            "\n",
            "BOCHENCIUS-I:\n",
            "I comman in he meblen to so, hone.\n",
            "\n",
            "Presist so my Hever!\n",
            "\n",
            "SOLDONUS:\n",
            "\n",
            "AURINIO:\n",
            "I prack him your not in sweal,\n",
            "For go hear thee restlect, then your a vone!\n",
            "\n",
            "PETER:\n",
            " ====================================================================================================\n",
            "tensor([1.7308], grad_fn=<DivBackward0>) \n",
            "\n",
            "bainst arts are:\n",
            "As brad a ress whe live ip son, thou and thy farecine.\n",
            "\n",
            "RASONS:\n",
            "Sour to the counrazbned entord of till pither if then folle and.\n",
            "\n",
            "SINCANA:\n",
            "Now thing that time you, my shall your gathin\n",
            " ====================================================================================================\n",
            "tensor([1.8973], grad_fn=<DivBackward0>) \n",
            "\n",
            "bure eartent must as tarroke the evessance.\n",
            "\n",
            "LEON EThas\n",
            "And mith, unour have your crame thou for and the proulion with my agay,\n",
            "That for forly love, the rases, lames, and my thou my faees am our the do\n",
            " ====================================================================================================\n",
            "tensor([1.6335], grad_fn=<DivBackward0>) \n",
            "\n",
            "ben, and friend, sown:\n",
            "I at the shall in the pascent and be thou thy winstrand, I wend her:\n",
            "He boof inherdy, with in, it he art but homen and know.\n",
            "\n",
            "BUCUSIS:\n",
            "A till and thou have this thou abe speak, a\n",
            " ====================================================================================================\n",
            "tensor([1.9025], grad_fn=<DivBackward0>) \n",
            "\n",
            "berven, so of head thoughts,--\n",
            "In follow his wis resouth our her as in but the will the manly.\n",
            "\n",
            "MENDUS:\n",
            "The hand kence, say, then toue to my shall in to arm be fiellow; never frief our\n",
            "bent badly! wise\n",
            " ====================================================================================================\n",
            "tensor([1.7425], grad_fn=<DivBackward0>) \n",
            "\n",
            "blance:\n",
            "Been, hear. Why, I have you will, sop of his worselly,\n",
            "We new repuil that then make endune?\n",
            "\n",
            "Forth a she your this come is my loard\n",
            "As have will virst have not strait:\n",
            "I halding what have is ha\n",
            " ====================================================================================================\n",
            "tensor([1.4852], grad_fn=<DivBackward0>) \n",
            "\n",
            "ble, has his bry will be body for ooth to helf,\n",
            "Be may, that it I what you eble she plaiss.\n",
            "\n",
            "CLITENTES Y LORY:\n",
            "See a latter, the fiintly as the pratus tay\n",
            "meas your first archard of you watfelffitling \n",
            " ====================================================================================================\n",
            "tensor([2.0507], grad_fn=<DivBackward0>) \n",
            "\n",
            "buty sparly.\n",
            "I have will true too will be you is more to the hame to man.\n",
            "\n",
            "ANTINES:\n",
            "Take love his elsh of he prace\n",
            "our shall dead in the lading you, process,\n",
            "For her dear remen the prior a paus\n",
            "The pra\n",
            " ====================================================================================================\n",
            "tensor([1.8435], grad_fn=<DivBackward0>) \n",
            "\n",
            "bice is, I shame suf\n",
            "My shear him go, I am frow this clame!\n",
            "\n",
            "ROMEO:\n",
            "Why, my some more more adman is so more less!\n",
            "\n",
            "ROMEO:\n",
            "And your co mannot we besue! I marrase that his live\n",
            "She lives fast in be, take\n",
            " ====================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tw-rv_oYgu0"
      },
      "source": [
        "## LSTM으로 수정하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehxfLZ1AbtWO"
      },
      "source": [
        "### 1) LSTM 모델 설계\n",
        "- LSTM은 RNN과 GRU와 다르게 훈련을 할 때 cell 상태를 이용합니다.\n",
        "- cell의 상태를 가지고 있으므로 모델을 만드는 부분을 변경하고 forward함수 및 초기화 함수도 수정해야 합니다.\n",
        "\n",
        "####  수정 부분 \n",
        "1. ```class RNN(nn.Module) : ```<br>\n",
        "    --> ```class LSTM(nn.Module) : ```\n",
        "2.  ```__init__```함수부분 수정\n",
        "    - ```super(RNN, self).__init__()``` <br>\n",
        "        --> ```super(LSTM, self).__init__()```\n",
        "    - ```self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers)```<br>\n",
        "        -->  ```self.rnn = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers)```\n",
        "\n",
        "3. ```forward(self, input, hidden)```함수 부분\n",
        "    - ```forward(self, input, hidden)```<br>\n",
        "        --> ```forward(self, input, hidden, cell)```\n",
        "    - ```out, hidden = self.rnn(out, hidden)```<br>\n",
        "        --> ```out, (hidden, cell) = self.rnn(out, (hidden, cell))```\n",
        "    - ```return out, hidden```<br>\n",
        "        --> ```return out, hidden, cell```\n",
        "4. ```init_hidden(self)``` 함수 부분\n",
        "    - ```cell = torch.zeros(self.num_layers, batch_size, self.hidden_size)``` 추가\n",
        "    - ```return hidden``` <br>\n",
        "        -->```return hidden, cell```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxWJfShZbtWU"
      },
      "source": [
        "class LSTM(nn.Module) : \n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
        "        super(LSTM, self).__init__()\n",
        "        # 인스턴스 변수 생성\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # 입력, 은닉, 출력층을 생성\n",
        "\n",
        "        # 출력: self.embedding_size\n",
        "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        # 위에 출력(self.embedding_size)을 입력으로 받음\n",
        "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers)\n",
        "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    # 순방향 훈련할 때 호출되는 함수\n",
        "    # nn.Module의 메소드를 overriding\n",
        "    def forward(self, input, hidden, cell):\n",
        "        # view가 Flatten역할\n",
        "        out = self.encoder(input.view(1, -1))\n",
        "        # LSTM은 cell 상태가 1개 추가\n",
        "        out, (hidden, cell) = self.lstm(out, (hidden, cell))\n",
        "        # hidden은 decoder로 넘기지 않고 out만 넘겨줌\n",
        "        out = self.decoder(out.view(batch_size, -1))\n",
        "        # return 도 cell추가 \n",
        "        return out, hidden, cell\n",
        "\n",
        "    #hidden값을 초기화해주는 함수 - cell 상태도 초기화해줌\n",
        "    # 사용자 정의 메소드\n",
        "    def init_hidden(self):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        return hidden, cell\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0BLgncobtWU"
      },
      "source": [
        "### 2) 모델 생성\n",
        "####  수정 부분 \n",
        "1. ```GRU ```<br>\n",
        "    --> ```LSTM```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyaMou0lbtWV"
      },
      "source": [
        "model = LSTM(input_size=n_characters, embedding_size=embedding_size, \n",
        "            hidden_size=hidden_size, output_size=n_characters,\n",
        "            num_layers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjqzuIn5btWV"
      },
      "source": [
        "### 3) 모델 확인\n",
        "\n",
        "####  수정 부분 \n",
        "1. ```hidden = model.init_hidden() ```<br>\n",
        "    --> ```hidden, cell = model.init_hidden()```\n",
        "\n",
        "2. ```out, hidden = model(inp, hidden)``` <br>\n",
        "    --> ```out, hidden, cell = model(inp, hidden, cell)```    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy5_Nu05btWV",
        "outputId": "1227acad-e8d7-4693-a0cc-b049ffdbe24a"
      },
      "source": [
        "inp = char_tensor('A')\n",
        "print(inp)\n",
        "\n",
        "# hidden = model.init_hidden()\n",
        "hidden, cell = model.init_hidden()\n",
        "print(hidden.size())\n",
        "\n",
        "# forward함수가 실행됨\n",
        "# out, hidden = model(inp, hidden)\n",
        "out, hidden, cell = model(inp, hidden, cell)\n",
        "print(out.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([36])\n",
            "torch.Size([2, 1, 100])\n",
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKUI5i-_gI9d"
      },
      "source": [
        "### 4) 하나의 문자를 대입하면 200짜리 텍스트를 생성해주는 함수\n",
        "1. ```hidden = model.init_hidden()```<br>\n",
        "    - ```hidden, cell = model.init_hidden()```\n",
        "2. ```output, hidden = model(x, hidden)``` <br>\n",
        "    - ```output, hidden, cell = model(x, hidden, cell)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnKMpmQ9gI9j"
      },
      "source": [
        "def lstm_test() :\n",
        "    start_str = 'b'\n",
        "    inp = char_tensor(start_str)\n",
        "    hidden, cell = model.init_hidden()\n",
        "    x = inp\n",
        "\n",
        "    # 중간결과는 출력문에서 안나오므로 출력해서 확인\n",
        "    print(start_str, end='')\n",
        "\n",
        "    # b입력 x, 상태 - x, 상태가 입력되서 x\n",
        "    for i in range(200):\n",
        "        output, hidden, cell = model(x, hidden, cell)\n",
        "        # tensor의 형태이므로 펼쳐냄\n",
        "        output_dist = output.data.view(-1).div(0.8).exp()\n",
        "\n",
        "        # 다음 글자를 꺼낼 때 max값을 꺼내면 매번 같은 값이 나오므로 \n",
        "        # 랜덤하게 뽑아내기 위해서 multinomial을 적용\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        \n",
        "        # 실제 글자 가져오기\n",
        "        predicated_char = all_characters[top_i]\n",
        "        print(predicated_char, end='')\n",
        "        x = char_tensor(predicated_char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQgxKRIRbtWV"
      },
      "source": [
        "### 5) 모델 훈련\n",
        "#### 수정부분\n",
        "- ```hiddenl = model.init_hidden()```<br>\n",
        "    -->```hidden, cell = model.init_hidden()```\n",
        "- ```y, hidden = model(x, hidden)```<br>\n",
        "    -->```y, hidden, cell = model(x, hidden, cell)```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj40mMLobtWV",
        "outputId": "ce525cbd-0062-44e2-fc07-c59352983764"
      },
      "source": [
        "# 최적화함수와 손실 함수 설정\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# 손실함수 : 크로스 엔트로피 \n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    # 랜덤한 문자열 가져오기\n",
        "    # inp:입력, label:목표\n",
        "    inp, label = random_training_set()\n",
        "    \n",
        "    # 은닉의 개수 초기화\n",
        "    hidden, cell = model.init_hidden()\n",
        "    \n",
        "    # 손실 함수 생성\n",
        "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
        "\n",
        "    # 미분값을 초기화\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for j in range(chunk_len -1 ):\n",
        "        x = inp[j]\n",
        "        # squeeze는 차원이 1인 차원을 제거\n",
        "        # unsqueeze는 차원을 늘려줍니다.\n",
        "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
        "        y, hidden, cell = model(x, hidden, cell)\n",
        "\n",
        "        # 손실은 y(예측값)와 y_(실제 레이블 값) 과의 차이로 계산\n",
        "        loss += loss_func(y, y_)\n",
        "\n",
        "    # 역전파\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(loss/chunk_len, '\\n')\n",
        "        lstm_test()\n",
        "        print('\\n', '=' * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([4.5528], grad_fn=<DivBackward0>) \n",
            "\n",
            "bP%j<lDm#>~SkTgg\tvo{efC1A8vm=7^T^z:<ajy&\tfF[+X*4KN47O|!6%\u000biEqqHtKDt(Y1bYKx,[]\u000b7CdXjm/\rBlU$|5DkSV]]4\u000b<:M S+<[;\rfqZRIIyiK4UaN#\fU/\rW|M|4ai*\"Ik,2Z-G{k\n",
            "iC :VTRuyLj1h$vr|p8Ur;|\fG\tL'bGM?Hx|#iX+-w87hjX%hg,l_M5\n",
            " ====================================================================================================\n",
            "tensor([3.1554], grad_fn=<DivBackward0>) \n",
            "\n",
            "b:nngre fh nsoo\n",
            "y nlrs e tonih eehro ami w d wIp\n",
            "f tnapnudv yoame lh be wse p se roa.n cd oinan ed t soiarg al nenteosdd tuFpeol woaae,e ld se tah, n,e ct pe eeuh fiat ay,nt&dl teeAenrr Dib\n",
            "d ny su s h\n",
            " ====================================================================================================\n",
            "tensor([2.6336], grad_fn=<DivBackward0>) \n",
            "\n",
            "bt tweeiw.\n",
            "Ornd oot aNoI simr hou Touy i wou sifos bassd\n",
            "AA wor heacerttenr tlemhe\n",
            "bes the rlort cot: any notrd thhet anto fuse, ey,\n",
            "\n",
            "BOI\n",
            "I:\n",
            "Phewlceel me fo yu, rareres loue met\n",
            "Ieleus then, ghans ad s\n",
            " ====================================================================================================\n",
            "tensor([2.4106], grad_fn=<DivBackward0>) \n",
            "\n",
            "bcat mot sagare to dhil athe, wim art rito thile'rut I tor urtr,\n",
            "Bive wile pot aing thor tin iunwof bat beit us wat anin.\n",
            "Ah meiny hes sok rot ht o to dewe boce si. me he auss the ther sve me,\n",
            "I:\n",
            "USKIR\n",
            " ====================================================================================================\n",
            "tensor([2.2765], grad_fn=<DivBackward0>) \n",
            "\n",
            "bthe gor! nereen epiln I he they eel ilen: be, no, hatruv, bervord vor tho shithid whers is reat coridesen,\n",
            "Gptill: I ir shir his pye gracerers fith co sis rethessly your\n",
            "fon of shas in wet rereeer the\n",
            " ====================================================================================================\n",
            "tensor([2.7047], grad_fn=<DivBackward0>) \n",
            "\n",
            "bre pod te the dame is ane mond yor to bere mar'g wer all nover in lemend we woule ple susbe you serpersord skomingt tud tho sonlemind nod thou go gisind wy th'd, ther, bou, thet the aldinar, to prarwe\n",
            " ====================================================================================================\n",
            "tensor([2.1600], grad_fn=<DivBackward0>) \n",
            "\n",
            "bbupond proungly toes ware and wlerewerd fiveay his tort ing, I four your shilnte, orterts aderert deenind pare ald to yetfer\n",
            "Pathou, thath klet angres, mielgee my my or is ans withes.'\n",
            "Fave not thot c\n",
            " ====================================================================================================\n",
            "tensor([2.0297], grad_fn=<DivBackward0>) \n",
            "\n",
            "bul ghile eord weat atsime ceala\n",
            "Whous lere ther me coumry ame thy ad tarmithour I knads to the sus the deardica thet whand theeit fore stily ther sind,\n",
            "The theatNoud\n",
            "\n",
            "Awfur theroud thirige, mart 'tor \n",
            " ====================================================================================================\n",
            "tensor([2.1145], grad_fn=<DivBackward0>) \n",
            "\n",
            "by I the for to thou, your,\n",
            "staw.\n",
            "\n",
            "GEENANA:\n",
            "I celhert,\n",
            "And a mow harm\n",
            "The fo the sing\n",
            "\n",
            "Fars ant mand mind of thou thas teblant's prartian me you fou?\n",
            "\n",
            "HESTEDRS:\n",
            "Hel the sut heiks wand sued wo con thou \n",
            " ====================================================================================================\n",
            "tensor([2.0537], grad_fn=<DivBackward0>) \n",
            "\n",
            "bat; torestred peargarder the bese memathes at brend:\n",
            "Shate tham lath adith.\n",
            "\n",
            "IARRKO:\n",
            "I, and bore, iges the rosting yourden me srooghesen, I mimive no the palather shand the thou gort\n",
            "Pongea'\n",
            "Tht foith\n",
            " ====================================================================================================\n",
            "tensor([2.0087], grad_fn=<DivBackward0>) \n",
            "\n",
            "bake that hen, the deeve beet harster ferlar, shour a rat what and that wiy tind\n",
            "This and whind ont.\n",
            "\n",
            "KINSLU\n",
            "CGAAA:\n",
            "Nat sing my what.\n",
            "\n",
            "ONKE:\n",
            "Po thou fine whecrtuling frot ulise the frill the to and nou\n",
            " ====================================================================================================\n",
            "tensor([2.0916], grad_fn=<DivBackward0>) \n",
            "\n",
            "be you' docstar the sere farte leel cove wice I there emrounthe te sheind for stroungs\n",
            "Old of what hestror.\n",
            "\n",
            "PYOLCIUND:\n",
            "A here here men it ghour hinger I that be the blout I ligh sour the lize love in \n",
            " ====================================================================================================\n",
            "tensor([2.1469], grad_fn=<DivBackward0>) \n",
            "\n",
            "be tey, of hould the sistend be sleen swea. Jome, and,\n",
            "I deir deant laen\n",
            "Jom he for yloon hand he hath bet, ko lelly,\n",
            "And so sid weat thes and to to lime seand beln the nons not dest the owly ti'\n",
            "Thiwg\n",
            " ====================================================================================================\n",
            "tensor([2.2358], grad_fn=<DivBackward0>) \n",
            "\n",
            "be the shis sill west in thou your mes ruer.\n",
            "\n",
            "KINANO:\n",
            "Frat gome;\n",
            "Yuls our witle is no, itler bay the plaed you had, all yous shat tals all or here counter, reith\n",
            "rend, prowe habt ge doy our to me all p\n",
            " ====================================================================================================\n",
            "tensor([2.0636], grad_fn=<DivBackward0>) \n",
            "\n",
            "bre here, for hom becle dousamy you all and the distch, broves, and lidt be the that you parth my my man, unclould he thee sent, in, wes lead weme to Het shes have to lat,\n",
            "The the nourgeng be knell! I \n",
            " ====================================================================================================\n",
            "tensor([1.9928], grad_fn=<DivBackward0>) \n",
            "\n",
            "beston unote then hage pill.\n",
            "\n",
            "SONICTO:\n",
            "Whis and how celd with she's them?\n",
            "\n",
            "MINCENY:\n",
            "The lors the tord, courdy sence I hiny ich, and paying no dealle.\n",
            "\n",
            "KINGLA:\n",
            "No hien your worll.\n",
            "\n",
            "GONLUS:\n",
            "I are Mad a u\n",
            " ====================================================================================================\n",
            "tensor([1.8071], grad_fn=<DivBackward0>) \n",
            "\n",
            "be in faruse barsy our to grime the thich!\n",
            "\n",
            "DASTO HIRK:\n",
            "Mest lood, there; whive.\n",
            "\n",
            "TRARTHONTIO:\n",
            "Ih it come.\n",
            "\n",
            "How arvOTsch sove fome:\n",
            "Do sechintont is and poneshe I here of old thead there, lith,\n",
            "And vep\n",
            " ====================================================================================================\n",
            "tensor([2.0153], grad_fn=<DivBackward0>) \n",
            "\n",
            "bord.\n",
            "Dobe all kerane sold, what for strunl, tence sones foe hatpord,\n",
            "And word out toiend,\n",
            "I my wirsiry then Gly you ushose to my it will wire hith from hy voweld;\n",
            "On and futh is and tele fet sir sord'\n",
            " ====================================================================================================\n",
            "tensor([1.7670], grad_fn=<DivBackward0>) \n",
            "\n",
            "brem, if steetion art are the solce thear wible, hid such there the me, there ame that of will in the gord so, wording a well of arver book fill wour whas merroms curscords on to manse there sin, what \n",
            " ====================================================================================================\n",
            "tensor([1.8414], grad_fn=<DivBackward0>) \n",
            "\n",
            "best of repo-\n",
            "Thee.\n",
            "\n",
            "BULIO:\n",
            "There moming! Deandy us to ther mere im unbanes oucame's of duke whant she nothince that, leake,\n",
            "In I the storsisn what of soome all of in me thy shind her in that.\n",
            "\n",
            "Nurcen:\n",
            " ====================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}