# CartPolê²Œì„ì„ ì´ìš©í•œ ê°•í™”í•™ìŠµ

break out(ë¸”ë¡ê¹¨ê¸°)ì€ ë£°ì´ ë³µì¡í•˜ì—¬ í›ˆë ¨í•˜ëŠ”ë° ì˜¤ë˜ê±¸ë¦½ë‹ˆë‹¤.

1. ê²Œì„ì˜ ê°œìš”

- ë§ˆì°°ì´ ì—†ëŠ” íŠ¸ë™ì— ì¹´íŠ¸ê°€ ì¡´ì¬
- ì¹´íŠ¸ì— ë§‰ëŒ€(pole)ê°€ ìˆìŒ
- ì¹´íŠ¸ë¥¼ ì™¼ìª½ì´ë‚˜ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë°€ ìˆ˜ ìˆìŒ
- ì´ë™ì„ í–ˆì„ ë•Œ ë§‰ëŒ€ê°€ ë„˜ì–´ì§€ì§€ ì•Šìœ¼ë©´ +1ì˜ ë³´ìƒì´ ì£¼ì–´ì§
- ì¢…ë£Œ ì¡°ê±´
  - ë§‰ëŒ€ê°€ ìˆ˜ì§ìœ¼ë¡œë¶€í„° 12ë„ ì´ìƒ ê¸°ìš¸ì–´ì§€ë©´ ì¢…ë£Œ
  - ì¹´íŠ¸ì˜ ì¤‘ì‹¬ìœ¼ë¡œë¶€í„° ê±°ë¦¬ê°€ -2.4 ~ 2.4ì‚¬ì´ì–´ì•¼ í•˜ëŠ”ë° ë©€ì–´ì§€ë©´ ì¢…ë£Œ
  - ìŠ¤í…ì´ 200ì´ ë˜ë©´ ì¢…ë£Œ
  - 100ë²ˆ ì´ìƒì˜ ì—°ì†ì ì¸ ì‹œë„ì—ì„œ 195ì  ì´ìƒì„ ë°›ìœ¼ë©´ ì¢…ë£Œ



# 1.Gym ë¼ì´ë¸ŒëŸ¬ë¦¬

- ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ê°œë°œí•˜ê³  ë¹„êµí•˜ê¸° ìœ„í•œ íˆ´í‚·
- TensorFlowì™€ Theano ì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í˜¸í™˜ê°€ëŠ¥í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ê°•í™”í•™ìŠµì„ í•  ê²ƒì´ë¼ë©´ TensorFlowë¥¼ í•˜ëŠ” ê²ƒì´ ì˜ˆì œë¥¼ ì‚¬ìš©í•˜ê¸°ì—ë„ ì¢‹ì„ ê²ƒì…ë‹ˆë‹¤.
- Gymë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ê°•í™” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•  í…ŒìŠ¤íŠ¸ ë¬¸ì œ(í™˜ê²½)ë“¤ì˜ ëª¨ì„ì´ë©° ì´ëŸ¬í•œ í™˜ê²½ë“¤ì„ ì´ìš©í•´ì„œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ê³µìœ í•´ì„œ í•™ìŠµì„ ìˆ˜í–‰í•´ ë³¼ ìˆ˜ ìˆìœ¼ë©° ì†ŒìŠ¤ë¥¼ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.



# 2.Gym ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë°©ë²•

## 1) pipë¥¼ ì´ìš©í•œ ë„¤íŠ¸ì›Œí¬ ì„¤ì¹˜

```
pip install gym
```



## 2) ì†ŒìŠ¤ë¥¼ ì´ìš©í•´ì„œ ì„¤ì¹˜

íšŒì‚¬ì—ì„œ ë§ì´ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

```
git clone https://github.com/openai/gym
cd gym
pip install -e
```



# 3. ê²Œì„ ì‹¤í–‰

ì†ŒìŠ¤íŒŒì¼ : [cartpole](./cartpole) ì˜ ğŸ“„main.pyë¥¼ ì‹¤í–‰



## 1) ê²Œì„ì‹¤í–‰

```python
# 1. ê²Œì„ì‹¤í–‰
import gym
env = gym.make('CartPole-v0')
env.reset()

for _ in range(1000):
    env.render()
    env.step(env.action_space.sample())
env.close()
```



## 2) Observations

- ì´ê²Œì„ì€  step()ì´ë¼ëŠ” í•¨ìˆ˜ê°€ 4ê°œì˜ í•„ìš”í•œ ê°’(ê´€ì°°, ë³´ìƒ ,ì¢…ë£Œì—¬ë¶€, ë¶€ê°€ ì •ë³´)ì„ ìˆœì„œëŒ€ë¡œ ë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤. 
  - object observation(ê´€ì°°) : í™˜ê²½ì— ëŒ€í•œ ê´€ì°°ì„ ë‚˜íƒ€ë‚´ëŠ” ê°ì²´, ê²Œì„ë§ˆë‹¤ ë‹¬ë¼ì§€ê²Œ ë©ë‹ˆë‹¤
  - float reword(ë³´ìƒ) : ì´ì „ì˜ í–‰ë™ìœ¼ë¡œ ì¸í•œ ë³´ìƒì˜ ì–‘, í¬ê¸°ëŠ” ê²Œì„ë§ˆë‹¤ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì§€ë§Œ ëª©í‘œëŠ” ë³´ìƒì˜ ì´ëŸ‰ì„ ë†’ì´ëŠ” ê²ƒì…ë‹ˆë‹¤
  - boolean done(ì¢…ë£Œ ì—¬ë¶€) : ì´ ê°’ì´ Trueì´ë©´ ëª©í‘œ ë‹¬ì„±ì…ë‹ˆë‹¤
  - dict info(ë¶€ê°€ ì •ë³´) : ê¸°íƒ€ ì •ë³´ë¥¼ ì „ë‹¬í•˜ê³ ì í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

- Agent -> í–‰ë™ì„ ì·¨í•¨ -> Environment ëŠ” ê´€ì°°ê³¼ ë³´ìƒì„ ë¦¬í„´ -> AgentëŠ” í–‰ë™ì„ ë°˜ë³µí•˜ëŠ” í˜•íƒœ

```python
# 2.Observationê°’ í™•ì¸
import gym
env = gym.make('CartPole-v0')
observation = env.reset()
# ì¹´íŠ¸ì˜ ìœ„ì¹˜, ì¹´íŠ¸ì˜ ì†ë„, ë§‰ëŒ€ê¸°ì˜ ê°ë„, ë§‰ëŒ€ê¸°ì˜ íšŒì „ìœ¨
# [ 0.01230672  0.01841324 -0.00403877 -0.04145269]
print(observation)
```



## 3) í–‰ë™ íŒŒì•…

`env.action_space.sample()`ì„ ì´ìš©í•´ì„œ í•˜ë‚˜ì˜ í–‰ë™ì„ ë¦¬í„´ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²Œì„ì€ ì¢Œìš°ë¡œë§Œ ì›ì§ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 0 ì•„ë‹ˆë©´ 1ì…ë‹ˆë‹¤.

```python
# 3. í–‰ë™ í™•ì¸ : ê²Œì„ì—ì„œ ì·¨í•  ìˆ˜ ìˆëŠ” ë™ì‘
import gym
env = gym.make('CartPole-v0')
observation = env.reset()
action = env.action_space.sample()
print(action)
```

- step(action)ì„ í˜¸ì¶œí•˜ë©´ í•œ ë²ˆì˜ í–‰ë™ì„ í•˜ê³  ê·¸ì— ë”°ë¥¸ ë³´ìƒì„ ë°›ê²Œ ë˜ëŠ”ë° ì´ ë•Œ ë¦¬í„´ë˜ëŠ” ë°ì´í„°ê°€ observation, reward, done, infoì…ë‹ˆë‹¤



## 4) step

 env.step()ì€ actionì„ ì„ íƒí–ˆì„ ë•Œ (observation, reward, done, info)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
# 4. í•œë²ˆì˜ ë™ì‘ì„ ìˆ˜í–‰í•˜ê³  ë™ì‘ì„ íŒŒì•…
import gym
env = gym.make('CartPole-v0')
observation = env.reset()
action = env.action_space.sample()
step = env.step(action)

# First observation: [ 0.03684896 -0.04953816 0.04790557 0.0064916 ]
print(observation)
# Action: 1
print(action)
# Step: (array([ 0.03585819, 0.14486519, 0.0480354 , -0.27070003]), 1.0, False, {}
print(step)
```

- ìƒ˜í”Œë§ ëœ í–‰ë™ì€ 1ì´ê³  ê·¸ë•Œì˜ observation[0.03585819, 0.14486519, 0.0480354 ,  -0.27070003])ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í•œ ì‹œê°„ ìŠ¤í… ë‹¹ ë³´ìƒ 1ì´ ì£¼ì–´ì§€ê³  ì•„ì§ ì—í”¼ì†Œë“œê°€ ì¢…ë£Œë˜ì§€ ì•Šì•˜ìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í–‰ë™ 0ê³¼ 1ì´ ê°ê° ì¹´íŠ¸ì— ì–´ëŠ ë°©í–¥ìœ¼ë¡œ í˜ì„ ê°€í•˜ëŠ”ì§€ í™•ì¸



## 5) ê²Œì„ ì§„í–‰

```python
# 5.ê²Œì„ ì§„í–‰
import gym
env = gym.make('CartPole-v0')
for i_episode in range(20):
    # ê²Œì„ í•  ë•Œë§ˆë‹¤ ë¦¬ì…‹
    observation = env.reset()
    # í•œë²ˆì˜ ê²Œì„ì„ 100ë²ˆì˜ ìŠ¤í…ìœ¼ë¡œ
    for t in range(100):
        # ëœë”ë§ - í™”ë©´ì— ì¶œë ¥
        env.render()
        action = env.action_space.sample()
        observation, reward, done, info = env.step(action)

        if done == True:
            print("episode finish {} timesteps".format(t+1))
            break
env.close()
```



![ezgif com-gif-maker](https://user-images.githubusercontent.com/58774664/132604501-435a9754-a1f5-4ebf-b6d1-6a95c8d45712.gif)

## 6) space

- í–‰ë™ ê³µê°„ìœ¼ë¡œ ë™ì‘ì´ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ê³µê°„ì´ ìˆê³  ê´€ì°°ì´ ê°€ëŠ¥í•œ ê³µê°„ì´ ìˆìŠµë‹ˆë‹¤. 
- ì´ ê²Œì„ì—ì„œëŠ” ì·¨í•  ìˆ˜ ìˆëŠ” ë™ì‘ì˜ ìˆ˜ì™€ 4ê°œì˜ ìˆ«ìë°°ì—´(ì¹´íŠ¸ì˜ ìœ„ì¹˜, ì¹´íŠ¸ì˜ ì†ë„, ë§‰ëŒ€ê¸°ì˜ ê°ë„, ë§‰ëŒ€ê¸°ì˜ íšŒì „ìœ¨)ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.

```python
# 6. ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ì˜ ë²”ìœ„
import gym
env = gym.make('CartPole-v0')
# ë™ì‘ ê³µê°„ : 2ê°€ì§€ ë™ì‘ì„ ì·¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ 2ê°€ ì¶œë ¥
# Discrete(2)
print(env.action_space)
# ê´€ì°° ê³µê°„ : ê´€ì°°ì˜ ë²”ìœ„
# Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)
print(env.observation_space)
```



## 7) ì•Œê³ ë¦¬ì¦˜ : í–‰ë™ 0ì˜ íš¨ê³¼í™•ì¸

- ì´ˆê¸° observationê°’ê³¼ í–‰ë™ì˜ 0ì˜ ê²½ìš°ì™€ ë¹„êµí•˜ë©´ ì¹´íŠ¸ì˜ ì†ë„ê°€ ì™¼ìª½ë°©í–¥ìœ¼ë¡œ ì¦ê°€í•˜ê³  ë§‰ëŒ€ê¸°ì˜ íšŒì „ìœ¨ì€ ì˜¤ë¥¸ìª½ ë°©í–¥ìœ¼ë¡œ ê¸°ìš¸ì—¬ì§‘ë‹ˆë‹¤.(ì¹´íŠ¸ì˜ ìœ„ì¹˜, ì¹´íŠ¸ì˜ ì†ë„, ë§‰ëŒ€ê¸°ì˜ ê°ë„, ë§‰ëŒ€ê¸°ì˜ íšŒì „ìœ¨)
  - `[-0.0132579   0.01739976  0.03656592 -0.02108136]` # ì´ˆê¸° ìƒíƒœ
  - `[-0.0129099  -0.17822699  0.03614429  0.28291059]`# í–‰ë™ 0ì˜ ê²½ìš°

```python
# 7. í–‰ë™ 0ì˜ íš¨ê³¼
import gym
env = gym.make('CartPole-v0')
obervation = env.reset()
# [-0.0132579   0.01739976  0.03656592 -0.02108136]
print(obervation)

# í–‰ë™ 0ì˜ ê²°ê³¼
obervation, reward, done, info = env.step(0)
# ì¹´íŠ¸ì˜ ìœ„ì¹˜, ì¹´íŠ¸ì˜ ì†ë„, ë§‰ëŒ€ê¸°ì˜ ê°ë„, ë§‰ëŒ€ê¸°ì˜ íšŒì „ìœ¨
# [-0.0129099  -0.17822699  0.03614429  0.28291059]
print(obervation)
env.close()
```



## 8) ì•Œê³ ë¦¬ì¦˜ : í–‰ë™ 0ì„ 100ë²ˆ ë°˜ë³µ

ì´ˆê¸°ê°’ ë¦¬ì…‹ -> í–‰ë™ 0ì„ 100íšŒ ë°˜ë³µí•©ë‹ˆë‹¤.

```python
# 8. í–‰ë™ 0ì˜ ë°˜ë³µ
import gym
env = gym.make('CartPole-v0')
obervation = env.reset()
for i in range(100):
    env.reset()
    # í–‰ë™ 0ìˆ˜í–‰
    obervation, reward, done, info = env.step(0)
    print(obervation, done)
    if done == True:
        break
env.close()
```



## 9) ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„

- í–‰ë™ 0ì€ ì†ë„ê°€ ì™¼ìª½ë°©í–¥ìœ¼ë¡œ ì¦ê°€í•˜ê³  ë§‰ëŒ€ê¸°ì˜ íšŒì „ìœ¨ì€ ì˜¤ë¥¸ìª½ ë°©í–¥ìœ¼ë¡œ ê¸°ìš¸ë„ë¡ í•©ë‹ˆë‹¤. 
- ì´ë¥¼ ì´ìš©í•˜ì—¬ ë§‰ëŒ€ê°€ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¸°ìš¸ì–´ì ¸ ìˆë‹¤ë©´ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í˜ì„ ê°€í•˜ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì™¼ìª½ìœ¼ë¡œ í˜ì„ ê°€í•˜ê¸°

```python
# 9. ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ ì„¤ê³„
import gym
env = gym.make('CartPole-v0')
obervation = env.reset()
for i in range(10000):
    env.reset()
    # obervation[2] : ë§‰ëŒ€ê¸°ì˜ ê°ë„
    # ë§‰ëŒ€ê°€ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê¸°ìš¸ì–´ì ¸ ìˆë‹¤ë©´ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í˜ì„ ê°€í•˜ê³  
    # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì™¼ìª½ìœ¼ë¡œ í˜ì„ ê°€í•˜ê¸°
    if obervation[2] > 0:
        action = 1
    else :
        action = 0
    obervation, reward, done, info = env.step(action)
    print(obervation, done)
    if done == True:
        print(i + 1)
        break
env.close()
```

`env.reset()`ì„ `env.render()`ë¡œ ë°”ê¾¸ê³  `if done == True : `ë¸”ëŸ­ì„ ì£¼ì„ì²˜ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ê°ë„ì— ë”°ë¼ í˜ì´ ê°€í•´ì§‘ë‹ˆë‹¤.

![ezgif com-gif-maker (1)](https://user-images.githubusercontent.com/58774664/132606836-4d5e55e5-dad5-4cdf-8ed1-dd5623a94822.gif)



## 10) ì¸ê³µ ì‹ ê²½ë§ ì´ìš©

```python
# 10. ì¸ê³µ ì‹ ê²½ë§ ì´ìš©
import gym
import tensorflow as tf
env = gym.make('CartPole-v0')
obervation = env.reset()

# ì¸ê³µ ì‹ ê²½ë§ ë§Œë“¤ê¸°
model = tf.keras.Sequential(
    [
        # ì…ë ¥ì¸µ
        tf.keras.layers.Dense(128, input_shape=(4,), activation=tf.nn.relu),
        # ì¶œë ¥ì¸µ
        tf.keras.layers.Dense(2)
    ]
)

# ì ìˆ˜ë¥¼ ì €ì¥í•  list
score = []
import numpy as np
# 100ë²ˆ ìˆ˜í–‰
for i in range(100):
    observation = env.reset()

    for t in range(200):
        # í›ˆë ¨í•´ì„œ ì˜ˆì¸¡
        predict = model.predict(observation.reshape(1, 4))
        # ì˜ˆì¸¡ì˜ ê²°ê³¼ë¥¼ ê°€ì§€ê³  ë‹¤ìŒ í–‰ë™ì„ ì„ íƒ
        action = np.argmax(predict)
        obervation, reward, done, info = env.step(action)
        if done == True:
            score.append(t + 1)
            break
env.close()
# [8, 9, 9, 9, 8, 8, 10, 10, 10, 9, 9, 10, 8, 8, 10, 9, 10, 10, 9, 10, 9, 10, 9, 9, 9, 10, 9, 9, 9, 9, 8, 9, 10, 10, 10, 10, 10, 11, 10, 9, 10, 9, 10, 10, 10, 8, 9, 10, 10, 10, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 9, 9, 10, 10, 9, 10, 8, 9, 10, 9, 10, 10, 10, 10, 10, 9, 10, 9, 9, 9, 10, 9, 10, 10, 9, 9, 10, 9, 10, 10, 9, 10, 9, 9, 9, 10, 9, 9, 10]
print(score)
```

scoreëŠ” ìˆ˜í–‰ íšŸìˆ˜ì¸ë° scoreì— ëŒ€í•œ ì°¨íŠ¸ë¥¼ ì¶œë ¥í•´ë³´ë©´ ë”¥ëŸ¬ë‹ì„ ìˆ˜í–‰í•´ë„ íšŸìˆ˜ê°€ ë§ì´ ëŠ˜ì–´ë‚˜ì§„ ì•ŠìŠµë‹ˆë‹¤.  ë‹¨ìˆœí•˜ê²Œ ë¨¸ì‹ ëŸ¬ë‹ì´ë‚˜ ë”¥ëŸ¬ë‹ì„ ìˆ˜í–‰í•œë‹¤ê³  í•´ì„œ ì ì  ë” ì¢‹ì•„ì§€ê±°ë‚˜ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ ë•Œë¬¸ì— ì¤‘ê°„ì— ë©ˆì¶”ëŠ” ê¸°ëŠ¥ì´ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.

```python
# ì ìˆ˜ì°¨íŠ¸ ì¶œë ¥
import matplotlib.pyplot as plt
plt.figure(figsize=(10,4))
plt.plot(score, label='score', lw=3, color='r')

plt.xlabel('Episodes')
plt.ylabel('Score')

plt.title('Scores')
plt.legend()
plt.show()
```

![Figure_1](https://user-images.githubusercontent.com/58774664/132608063-3496a3f6-a2d4-4b4b-a522-ee0c255f5b5d.png)





## 11) ê°•í™”í•™ìŠµ ì ìš©

ë” í° ë³´ìƒì´ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í˜„ì¬ìœ„ì¹˜ì—ì„œì˜ í°ìª½ìœ¼ë¡œ ë¬´ì¡°ê±´ ì´ë™í•˜ë©´ ì•ˆë©ë‹ˆë‹¤. ê·¸ë˜ì„œ ë‚˜ì˜¨ê²Œ ì§€ì—°ëœ ë³´ìƒì…ë‹ˆë‹¤. 

ì¼ì • í™•ë¥ ì„  ë¬´ì¡°ê±´ ì•„ë˜ë¡œ ë‚´ë ¤ê°€ëŠ”ê²Œ ì•„ë‹ˆë¼ ë³´ìƒì´ ì ì€ ìª½ë„ ê°€ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì´ë¬¸ì œë¥¼ ê°•í™”í•™ìŠµì„ ë°”ê¿”ë³´ê² ìŠµë‹ˆë‹¤.

```python
# 11. ê°•í™”í•™ìŠµ ì ìš©
import gym
import tensorflow as tf
from tensorflow.keras.optimizers import Adam

import numpy as np
import random

from collections import deque

import matplotlib.pyplot as plt

# ì¸ê³µ ì‹ ê²½ë§ ë§Œë“¤ê¸°
model = tf.keras.Sequential(
    [
        # í›ˆë ¨ ì†ë„ë¥¼ ë¹ ë¥´ê²Œ í•˜ê¸° ìœ„í•´ ë‰´ëŸ°ì˜ ê°œìˆ˜ë¥¼ 24ê°œë¡œ ë³€ê²½
        # 4ê°œì˜ ì…ë ¥ê³¼ 2ê°œì˜ ì¶œë ¥ìœ¼ë¡œ ì´ë£¨ì–´ì§
        tf.keras.layers.Dense(24, input_dim=4, activation=tf.nn.relu),
        tf.keras.layers.Dense(24, activation=tf.nn.relu),
        tf.keras.layers.Dense(2, activation='linear')
])

# ìµœì í™”,í•™ìŠµìœ¨ì„¤ì •. ì†ì‹¤í•¨ìˆ˜=MSE
model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')

# ì •ë³´ë¥¼ ì €ì¥í•  ìë£Œêµ¬ì¡°ë¥¼ ìƒì„±
score=[]
# dequeë¡œ ë©”ëª¨ë¦¬ì œí•œì„ ë‘¡ë‹ˆë‹¤.
memory = deque(maxlen=2000)

env = gym.make('CartPole-v0')

for i in range(1000):
    state = env.reset()
    state = np.reshape(state, [1, 4])
    # ì—¡ì‹¤ë¡  ê°’ : ìˆ˜í–‰í•  ë•Œë§ˆë‹¤ íƒìƒ‰ë³´ë‹¤ í™œìš©ì˜ ë¹„ìœ¨ì´ ë†’ì•„ì§€ë„ë¡ í•¨
    eps = 1 / (i / 50 + 10)
    
    # ê° ìˆ˜í–‰ë§ˆë‹¤ í–‰ë™ì„ ë‹¤ë¥´ê²Œ ì„¤ì •
    for t in range(200):
        # ì—¡ì‹¤ë¡ ë³´ë‹¤ ì‘ìœ¼ë©´ ëœë¤í•œ ë™ì‘ì„ ìˆ˜í–‰í•˜ê³ 
        # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë³´ìƒì´ ë†’ì€ëŒ€ë¡œ í›ˆë ¨
        if np.random.rand() < eps:
            # ë™ì‘ì„ ëœë¤í•˜ê²Œ ì„¤ì •
            action = np.random.randint(0, 2)
        else :
            predict = model.predict(state)
            action = np.argmax(predict)
        next_state, reward, done, _ = env.step(action)
        next_state = np.reshape(next_state, [1, 4])

        # ê²°ê³¼(í˜„ì¬ ìƒíƒœ,í–‰ë™, ë‹¤ìŒ ìƒíƒœ, ë³´ìƒ, done)ì„ íŠœí”Œì˜ í˜•íƒœë¡œ memoryì— ì €ì¥
        memory.append((state, action, reward, next_state, done))
        # ìƒíƒœ(state)ë¥¼ ë‹¤ìŒ ìƒíƒœ (next_state)ë¡œ ì „í™˜
        state = next_state

    # 10ë²ˆ ì´ìƒì˜ ì—í”¼ì†Œë“œë¥¼ ì§„í–‰í•˜ë©° í›ˆë ¨
    if i > 10:
        minibatch = random.sample(memory, 16)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done :
                # ì™„ë£Œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ê°ê°€ìœ¨ ê³„ì‚°
                target = reward + 0.9 * np.amax(model.predict(next_state)[0])
            target_outputs = model.predict(state)
            target_outputs[0][action] = target
            # í›ˆë ¨
            model.fit(state, target_outputs, epochs=1, verbose=0)

env.close()
print(score)
```

ìˆ˜í–‰í•  ìˆ˜ë¡ ìŠ¤ì½”ì–´ê°€ ì¢‹ì•„ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.





# ì¼ë°˜ì ì¸ ê¸°ê³„í•™ìŠµê³¼ ê°•í™”í•™ìŠµì˜ ì°¨ì´



## 1.ì¼ë°˜ì ì¸ ê¸°ê³„í•™ìŠµ

- ì¼ë°˜ì ì¸ ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ ë°ì´í„°ì— ì˜í–¥ì„ ë§ì´ ë°›ëŠ”ë° ë°˜ë³µíšŸìˆ˜ë‚˜ ì—í”¼ì†Œë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ê¸° ë•Œë¬¸ì— íŠ¹ì • ë°˜ë³µ íšŸìˆ˜ë‚˜ ì—í”¼ì†Œë“œë¥¼ ë„˜ì–´ê°€ê²Œ ë˜ë©´ ë”ì´ìƒ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤. ì„±ëŠ¥ì„ ì¢‹ê²Œ í•˜ë ¤ë©´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ë‚˜ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.





## 2.ê°•í™”í•™ìŠµ

- ê°•í™”í•™ìŠµì€ ë°ì´í„°ë¼ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëŒ€ì‹ ì— í™˜ê²½ì´ë¼ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•˜ëŠ”ë° í™˜ê²½ì€ ê³ ì •ë˜ì–´ìˆìŠµë‹ˆë‹¤. ì—í”¼ì†Œë“œë¥¼ ì§„í–‰í•˜ë©´ ì´ì „ ì—í”¼ì†Œë“œì— ì˜í–¥ì„ ë°›ì•„ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ë‹¤ë¥¸ ì—í”¼ì†Œë“œì™€ ë¹„êµí•´ì„œ ê°€ì¥ ì¢‹ì€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì°¾ê¸°ë„ í•˜ê³  ì´ì „ ì—í”¼ì†Œë“œì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸°ë„ í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì—í”¼ì†Œë“œì˜ ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ì„±ëŠ¥ì´ ì¢‹ì•„ì§‘ë‹ˆë‹¤.
